{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=dodgerblue> **Question # 1 Eng-Urdu Translation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  <font color=RoyalBlue>  **Loading Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Define base directory for corpus\n",
    "base_dir = \"umc005-corpus\"\n",
    "quran_dir = os.path.join(base_dir, \"quran\")\n",
    "bible_dir = os.path.join(base_dir, \"bible\")\n",
    "\n",
    "# Function to load parallel data from given file paths\n",
    "def load_parallel_data(en_path, ur_path):\n",
    "    with open(en_path, \"r\", encoding=\"utf-8\") as en_file, open(ur_path, \"r\", encoding=\"utf-8\") as ur_file:\n",
    "        english_sentences = en_file.readlines()\n",
    "        urdu_sentences = ur_file.readlines()\n",
    "\n",
    "    assert len(english_sentences) == len(urdu_sentences), \"Mismatch in number of lines between English and Urdu files.\"\n",
    "    return list(zip(english_sentences, urdu_sentences))\n",
    "\n",
    "# Load Quran data\n",
    "quran_train_data = load_parallel_data(os.path.join(quran_dir, \"train.en\"), os.path.join(quran_dir, \"train.ur\"))\n",
    "quran_test_data = load_parallel_data(os.path.join(quran_dir, \"test.en\"), os.path.join(quran_dir, \"test.ur\"))\n",
    "quran_dev_data = load_parallel_data(os.path.join(quran_dir, \"dev.en\"), os.path.join(quran_dir, \"dev.ur\"))\n",
    "\n",
    "# Load Bible data\n",
    "bible_train_data = load_parallel_data(os.path.join(bible_dir, \"train.en\"), os.path.join(bible_dir, \"train.ur\"))\n",
    "bible_test_data = load_parallel_data(os.path.join(bible_dir, \"test.en\"), os.path.join(bible_dir, \"test.ur\"))\n",
    "bible_dev_data = load_parallel_data(os.path.join(bible_dir, \"dev.en\"), os.path.join(bible_dir, \"dev.ur\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  <font color=RoyalBlue>  **Preprocessing & Tokenizing Dataset**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Have Used `Byte-Pair Encoding` (BPE) to tokenize urdu sentences parallel to english. <br/>Hugging Face’s SentencePieceTokenizer is BPE-based and can be trained on thr provided dataset. <br/> This approach can yield robust tokenization for languages with larger vocabularies or specific characters, like Urdu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sentencepiece in c:\\users\\javer\\anaconda3\\envs\\pymc5_env\\lib\\site-packages (0.2.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def clean_english(text):\n",
    "    text = text.strip().lower()  # Lowercase and remove leading/trailing spaces\n",
    "    # text = re.sub(r'[^\\w\\s]', '', text)  # Remove non-alphanumeric characters\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized text: ['▁', 'S', 'am', 'ple', '▁te', 'xt', '▁in', '▁', 'E', 'ng', 'lish', '▁or', '▁', 'U', 'rd', 'u', '▁for', '▁token', 'iz', 'ation']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sentencepiece as spm\n",
    "\n",
    "# Define base directory for the UMC005 corpus\n",
    "base_dir = \"umc005-corpus\"\n",
    "quran_dir = os.path.join(base_dir, \"quran\")\n",
    "bible_dir = os.path.join(base_dir, \"bible\")\n",
    "\n",
    "# Function to load parallel data from given file paths\n",
    "def load_parallel_data(en_path, ur_path):\n",
    "    with open(en_path, \"r\", encoding=\"utf-8\") as en_file, open(ur_path, \"r\", encoding=\"utf-8\") as ur_file:\n",
    "        english_sentences = en_file.readlines()\n",
    "        urdu_sentences = ur_file.readlines()\n",
    "    assert len(english_sentences) == len(urdu_sentences), \"Mismatch in number of lines between English and Urdu files.\"\n",
    "    return list(zip(english_sentences, urdu_sentences))\n",
    "\n",
    "# Collecting all parallel data pairs\n",
    "def collect_parallel_data():\n",
    "    data_pairs = []\n",
    "    quran_dataset = load_parallel_data(os.path.join(quran_dir, \"Quran-EN\"), os.path.join(quran_dir, \"Quran-UR-normalized\"))\n",
    "    bible_dataset = load_parallel_data(os.path.join(bible_dir, \"Bible-EN\"), os.path.join(bible_dir, \"Bible-UR-normalized\"))\n",
    "    # quran_train = load_parallel_data(os.path.join(quran_dir, \"train.en\"), os.path.join(quran_dir, \"train.ur\"))\n",
    "    # quran_test = load_parallel_data(os.path.join(quran_dir, \"test.en\"), os.path.join(quran_dir, \"test.ur\"))\n",
    "    # quran_dev = load_parallel_data(os.path.join(quran_dir, \"dev.en\"), os.path.join(quran_dir, \"dev.ur\"))\n",
    "    # bible_train = load_parallel_data(os.path.join(bible_dir, \"train.en\"), os.path.join(bible_dir, \"train.ur\"))\n",
    "    # bible_test = load_parallel_data(os.path.join(bible_dir, \"test.en\"), os.path.join(bible_dir, \"test.ur\"))\n",
    "    # bible_dev = load_parallel_data(os.path.join(bible_dir, \"dev.en\"), os.path.join(bible_dir, \"dev.ur\"))\n",
    "    \n",
    "    data_pairs.extend(quran_dataset + bible_dataset)\n",
    "    return data_pairs\n",
    "\n",
    "# Combine parallel data into a single file\n",
    "def create_parallel_training_file(data_pairs, output_file):\n",
    "    with open(output_file, \"w\", encoding=\"utf-8\") as out_file:\n",
    "        for en_sentence, ur_sentence in data_pairs:\n",
    "            cleaned_en = clean_english(en_sentence)  # Clean and lowercase English text\n",
    "            out_file.write(cleaned_en + \"\\n\")  # Write English sentence\n",
    "            out_file.write(ur_sentence.strip() + \"\\n\")  # Write corresponding Urdu sentence, already normalized\n",
    "\n",
    "# Preparing the parallel data for training\n",
    "parallel_data = collect_parallel_data()\n",
    "combined_parallel_file = \"combined_parallel_urdu_english.txt\"\n",
    "create_parallel_training_file(parallel_data, combined_parallel_file)\n",
    "\n",
    "# Train SentencePiece with the parallel data file\n",
    "spm.SentencePieceTrainer.train(\n",
    "    input=combined_parallel_file,\n",
    "    model_prefix=\"urdu_english_bpe_parallel\",\n",
    "    vocab_size=32000,  # Define a suitable vocabulary size\n",
    "    model_type=\"bpe\",  # Using Byte-Pair Encoding for tokenization\n",
    "    character_coverage=0.9995,  # Ensures all characters are covered\n",
    "    input_sentence_size=50000,  # Limit to 1M lines for faster training (adjust as needed)\n",
    "    shuffle_input_sentence=True\n",
    ")\n",
    "\n",
    "# Loading the trained tokenizer\n",
    "sp = spm.SentencePieceProcessor(model_file=\"urdu_english_bpe_parallel.model\")\n",
    "\n",
    "# Sample tokenization\n",
    "sample_text = \"Sample text in English or Urdu for tokenization\"\n",
    "tokenized_text = sp.encode(sample_text, out_type=str)\n",
    "print(\"Tokenized text:\", tokenized_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=\"seagreen\"> **Transformer Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Layers of Transformer Model*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining Positional Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.encoding = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * -(math.log(10000.0) / d_model))\n",
    "\n",
    "        self.encoding[:, 0::2] = torch.sin(position * div_term)\n",
    "        self.encoding[:, 1::2] = torch.cos(position * div_term)\n",
    "        self.encoding = self.encoding.unsqueeze(0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.encoding[:, :x.size(1), :].to(x.device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multi-Head Attention Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\"\n",
    "        self.d_k = d_model // num_heads\n",
    "        self.num_heads = num_heads\n",
    "\n",
    "        self.q_linear = nn.Linear(d_model, d_model)\n",
    "        self.k_linear = nn.Linear(d_model, d_model)\n",
    "        self.v_linear = nn.Linear(d_model, d_model)\n",
    "        self.out = nn.Linear(d_model, d_model)\n",
    "\n",
    "    def forward(self, q, k, v, mask=None):\n",
    "        batch_size = q.size(0)\n",
    "\n",
    "        # Linear projections\n",
    "        q = self.q_linear(q).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        k = self.k_linear(k).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        v = self.v_linear(v).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
    "\n",
    "        # Scaled Dot-Product Attention\n",
    "        scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(self.d_k)\n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask == 0, -1e9)\n",
    "        attn = F.softmax(scores, dim=-1)\n",
    "        output = torch.matmul(attn, v)\n",
    "\n",
    "        # Concatenate heads\n",
    "        output = output.transpose(1, 2).contiguous().view(batch_size, -1, self.num_heads * self.d_k)\n",
    "        return self.out(output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feedforward Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, d_model, d_ff, dropout=0.1):\n",
    "        super(FeedForward, self).__init__()\n",
    "        self.linear1 = nn.Linear(d_model, d_ff)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.linear2 = nn.Linear(d_ff, d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear2(self.dropout(F.relu(self.linear1(x))))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Encoder Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.self_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.ff = FeedForward(d_model, d_ff, dropout)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        attn_output = self.self_attn(x, x, x, mask)\n",
    "        x = self.norm1(x + self.dropout(attn_output))\n",
    "        ff_output = self.ff(x)\n",
    "        x = self.norm2(x + self.dropout(ff_output))\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decoder Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        self.self_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.enc_dec_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.ff = FeedForward(d_model, d_ff, dropout)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.norm3 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, enc_output, src_mask=None, tgt_mask=None):\n",
    "        self_attn_output = self.self_attn(x, x, x, tgt_mask)\n",
    "        x = self.norm1(x + self.dropout(self_attn_output))\n",
    "\n",
    "        enc_dec_attn_output = self.enc_dec_attn(x, enc_output, enc_output, src_mask)\n",
    "        x = self.norm2(x + self.dropout(enc_dec_attn_output))\n",
    "\n",
    "        ff_output = self.ff(x)\n",
    "        x = self.norm3(x + self.dropout(ff_output))\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Encoder Itself"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, num_heads, num_layers, d_ff, max_len, dropout=0.1):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.pos_encoding = PositionalEncoding(d_model, max_len)\n",
    "        self.layers = nn.ModuleList([EncoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)])\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        x = self.embedding(x) * math.sqrt(self.embedding.embedding_dim)\n",
    "        x = self.pos_encoding(x)\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, mask)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decoder Itself"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, num_heads, num_layers, d_ff, max_len, dropout=0.1):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.pos_encoding = PositionalEncoding(d_model, max_len)\n",
    "        self.layers = nn.ModuleList([DecoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)])\n",
    "\n",
    "    def forward(self, x, enc_output, src_mask=None, tgt_mask=None):\n",
    "        x = self.embedding(x) * math.sqrt(self.embedding.embedding_dim)\n",
    "        x = self.pos_encoding(x)\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, enc_output, src_mask, tgt_mask)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Complete Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, src_vocab_size, tgt_vocab_size, src_pad_idx, tgt_pad_idx, d_model, num_heads, num_layers, d_ff, max_len, dropout=0.1):\n",
    "        super(Transformer, self).__init__()\n",
    "        self.src_pad_idx = src_pad_idx\n",
    "        self.tgt_pad_idx = tgt_pad_idx\n",
    "        self.encoder = Encoder(src_vocab_size, d_model, num_heads, num_layers, d_ff, max_len, dropout)\n",
    "        self.decoder = Decoder(tgt_vocab_size, d_model, num_heads, num_layers, d_ff, max_len, dropout)\n",
    "        self.fc_out = nn.Linear(d_model, tgt_vocab_size)\n",
    "\n",
    "    def make_src_mask(self, src):\n",
    "        # Create mask for source\n",
    "        return (src != self.src_pad_idx).unsqueeze(1).unsqueeze(2)\n",
    "\n",
    "    def make_tgt_mask(self, tgt):\n",
    "        # Create mask for target (with causal mask for decoding)\n",
    "        tgt_pad_mask = (tgt != self.tgt_pad_idx).unsqueeze(1).unsqueeze(2)\n",
    "        tgt_len = tgt.size(1)\n",
    "        tgt_subseq_mask = torch.tril(torch.ones((tgt_len, tgt_len), device=tgt.device)).bool()\n",
    "        return tgt_pad_mask & tgt_subseq_mask\n",
    "\n",
    "    def forward(self, src, tgt):\n",
    "        src_mask = self.make_src_mask(src)\n",
    "        tgt_mask = self.make_tgt_mask(tgt)\n",
    "        enc_output = self.encoder(src, src_mask)\n",
    "        dec_output = self.decoder(tgt, enc_output, src_mask, tgt_mask)\n",
    "        output = self.fc_out(dec_output)\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sentencepiece as spm\n",
    "import torch\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "# Load the trained tokenizer\n",
    "tokenizer = spm.SentencePieceProcessor()\n",
    "tokenizer.load(\"urdu_english_bpe_parallel.model\")\n",
    "\n",
    "# Define max sequence length and pad index\n",
    "MAX_SEQ_LEN = 50\n",
    "PAD_IDX = tokenizer.piece_to_id(\"<pad>\")\n",
    "\n",
    "# Function to tokenize and pad data\n",
    "def tokenize_and_prepare(sentences):\n",
    "    tokenized = [tokenizer.encode(sentence, out_type=int) for sentence in sentences]\n",
    "    padded = [seq[:MAX_SEQ_LEN] + [PAD_IDX] * (MAX_SEQ_LEN - len(seq)) if len(seq) < MAX_SEQ_LEN else seq[:MAX_SEQ_LEN] for seq in tokenized]\n",
    "    return padded\n",
    "\n",
    "# Load English and Urdu sentences from files\n",
    "def load_sentences(file_path):\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        return f.readlines()\n",
    "\n",
    "# Load train data\n",
    "train_en_sentences = load_sentences(\"umc005-corpus/quran/train.en\") + load_sentences(\"umc005-corpus/bible/train.en\")\n",
    "train_ur_sentences = load_sentences(\"umc005-corpus/quran/train.ur\") + load_sentences(\"umc005-corpus/bible/train.ur\")\n",
    "\n",
    "# Tokenize and prepare data\n",
    "src_data = tokenize_and_prepare(train_en_sentences)\n",
    "tgt_data = tokenize_and_prepare(train_ur_sentences)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TranslationDataset(Dataset):\n",
    "    def __init__(self, src, tgt):\n",
    "        self.src = src\n",
    "        self.tgt = tgt\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.src)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return torch.tensor(self.src[idx]), torch.tensor(self.tgt[idx])\n",
    "\n",
    "# Create dataset and dataloader\n",
    "dataset = TranslationDataset(src_data, tgt_data)\n",
    "dataloader = DataLoader(dataset, batch_size=2, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyperparameters and Model Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model hyperparameters\n",
    "d_model = 128\n",
    "num_heads = 2\n",
    "num_layers = 2\n",
    "d_ff = 512\n",
    "max_len = 200\n",
    "dropout = 0.1\n",
    "\n",
    "src_vocab_size = tokenizer.get_piece_size()\n",
    "tgt_vocab_size = tokenizer.get_piece_size()\n",
    "\n",
    "    \n",
    "# Initialize the Transformer model\n",
    "# Assuming PAD_IDX is already defined, make sure both are passed:\n",
    "model = Transformer(\n",
    "    src_vocab_size=src_vocab_size,\n",
    "    tgt_vocab_size=tgt_vocab_size,\n",
    "    src_pad_idx=PAD_IDX,  # src padding index\n",
    "    tgt_pad_idx=PAD_IDX,  # tgt padding index\n",
    "    d_model=d_model,\n",
    "    num_heads=num_heads,\n",
    "    num_layers=num_layers,\n",
    "    d_ff=d_ff,\n",
    "    max_len=max_len,\n",
    "    dropout=dropout\n",
    ")\n",
    "\n",
    "\n",
    "# Define loss function with padding ignored\n",
    "criterion = torch.nn.CrossEntropyLoss(ignore_index=PAD_IDX)\n",
    "\n",
    "# Optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Training Loop** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20, Loss: 5.847795763905369\n",
      "Epoch 2/20, Loss: 4.914523594290463\n",
      "Epoch 3/20, Loss: 4.5751000347600055\n",
      "Epoch 4/20, Loss: 4.352262926599873\n",
      "Epoch 5/20, Loss: 4.184005458959892\n",
      "Epoch 6/20, Loss: 4.042845732934439\n",
      "Epoch 7/20, Loss: 3.918032662388104\n",
      "Epoch 8/20, Loss: 3.8128471590867683\n",
      "Epoch 9/20, Loss: 3.7139017724457073\n",
      "Epoch 10/20, Loss: 3.621993897566155\n",
      "Epoch 11/20, Loss: 3.534025015350598\n",
      "Epoch 12/20, Loss: 3.4561936408325806\n",
      "Epoch 13/20, Loss: 3.383977654051425\n",
      "Epoch 14/20, Loss: 3.312921400176945\n",
      "Epoch 15/20, Loss: 3.2490196408353635\n",
      "Epoch 16/20, Loss: 3.184731990588245\n",
      "Epoch 17/20, Loss: 3.1271846369531615\n",
      "Epoch 18/20, Loss: 3.0695784538717414\n",
      "Epoch 19/20, Loss: 3.020777788633731\n",
      "Epoch 20/20, Loss: 2.9712336649378734\n"
     ]
    }
   ],
   "source": [
    "# Set device to GPU if available, otherwise use CPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Move the model to the appropriate device\n",
    "model.to(device)\n",
    "\n",
    "\n",
    "num_epochs = 20\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for src_batch, tgt_batch in dataloader:\n",
    "        # Get input and target sequences\n",
    "        src_input = src_batch.to(device)\n",
    "        tgt_input = tgt_batch[:, :-1].to(device)  # Exclude last token\n",
    "        tgt_output = tgt_batch[:, 1:].to(device)  # Exclude first token\n",
    "\n",
    "        # Forward pass\n",
    "        logits = model(src_input, tgt_input)\n",
    "\n",
    "        # Compute loss\n",
    "        loss = criterion(logits.reshape(-1, logits.size(-1)), tgt_output.reshape(-1))\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {total_loss / len(dataloader)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"transformer_translation1.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\javer\\AppData\\Local\\Temp\\ipykernel_10056\\2754070716.py:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"transformer_translation1.pth\", map_location=device), strict=False)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(\"transformer_translation1.pth\", map_location=device), strict=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "odict_keys(['encoder.embedding.weight', 'encoder.layers.0.self_attn.q_linear.weight', 'encoder.layers.0.self_attn.q_linear.bias', 'encoder.layers.0.self_attn.k_linear.weight', 'encoder.layers.0.self_attn.k_linear.bias', 'encoder.layers.0.self_attn.v_linear.weight', 'encoder.layers.0.self_attn.v_linear.bias', 'encoder.layers.0.self_attn.out.weight', 'encoder.layers.0.self_attn.out.bias', 'encoder.layers.0.ff.linear1.weight', 'encoder.layers.0.ff.linear1.bias', 'encoder.layers.0.ff.linear2.weight', 'encoder.layers.0.ff.linear2.bias', 'encoder.layers.0.norm1.weight', 'encoder.layers.0.norm1.bias', 'encoder.layers.0.norm2.weight', 'encoder.layers.0.norm2.bias', 'encoder.layers.1.self_attn.q_linear.weight', 'encoder.layers.1.self_attn.q_linear.bias', 'encoder.layers.1.self_attn.k_linear.weight', 'encoder.layers.1.self_attn.k_linear.bias', 'encoder.layers.1.self_attn.v_linear.weight', 'encoder.layers.1.self_attn.v_linear.bias', 'encoder.layers.1.self_attn.out.weight', 'encoder.layers.1.self_attn.out.bias', 'encoder.layers.1.ff.linear1.weight', 'encoder.layers.1.ff.linear1.bias', 'encoder.layers.1.ff.linear2.weight', 'encoder.layers.1.ff.linear2.bias', 'encoder.layers.1.norm1.weight', 'encoder.layers.1.norm1.bias', 'encoder.layers.1.norm2.weight', 'encoder.layers.1.norm2.bias', 'decoder.embedding.weight', 'decoder.layers.0.self_attn.q_linear.weight', 'decoder.layers.0.self_attn.q_linear.bias', 'decoder.layers.0.self_attn.k_linear.weight', 'decoder.layers.0.self_attn.k_linear.bias', 'decoder.layers.0.self_attn.v_linear.weight', 'decoder.layers.0.self_attn.v_linear.bias', 'decoder.layers.0.self_attn.out.weight', 'decoder.layers.0.self_attn.out.bias', 'decoder.layers.0.enc_dec_attn.q_linear.weight', 'decoder.layers.0.enc_dec_attn.q_linear.bias', 'decoder.layers.0.enc_dec_attn.k_linear.weight', 'decoder.layers.0.enc_dec_attn.k_linear.bias', 'decoder.layers.0.enc_dec_attn.v_linear.weight', 'decoder.layers.0.enc_dec_attn.v_linear.bias', 'decoder.layers.0.enc_dec_attn.out.weight', 'decoder.layers.0.enc_dec_attn.out.bias', 'decoder.layers.0.ff.linear1.weight', 'decoder.layers.0.ff.linear1.bias', 'decoder.layers.0.ff.linear2.weight', 'decoder.layers.0.ff.linear2.bias', 'decoder.layers.0.norm1.weight', 'decoder.layers.0.norm1.bias', 'decoder.layers.0.norm2.weight', 'decoder.layers.0.norm2.bias', 'decoder.layers.0.norm3.weight', 'decoder.layers.0.norm3.bias', 'decoder.layers.1.self_attn.q_linear.weight', 'decoder.layers.1.self_attn.q_linear.bias', 'decoder.layers.1.self_attn.k_linear.weight', 'decoder.layers.1.self_attn.k_linear.bias', 'decoder.layers.1.self_attn.v_linear.weight', 'decoder.layers.1.self_attn.v_linear.bias', 'decoder.layers.1.self_attn.out.weight', 'decoder.layers.1.self_attn.out.bias', 'decoder.layers.1.enc_dec_attn.q_linear.weight', 'decoder.layers.1.enc_dec_attn.q_linear.bias', 'decoder.layers.1.enc_dec_attn.k_linear.weight', 'decoder.layers.1.enc_dec_attn.k_linear.bias', 'decoder.layers.1.enc_dec_attn.v_linear.weight', 'decoder.layers.1.enc_dec_attn.v_linear.bias', 'decoder.layers.1.enc_dec_attn.out.weight', 'decoder.layers.1.enc_dec_attn.out.bias', 'decoder.layers.1.ff.linear1.weight', 'decoder.layers.1.ff.linear1.bias', 'decoder.layers.1.ff.linear2.weight', 'decoder.layers.1.ff.linear2.bias', 'decoder.layers.1.norm1.weight', 'decoder.layers.1.norm1.bias', 'decoder.layers.1.norm2.weight', 'decoder.layers.1.norm2.bias', 'decoder.layers.1.norm3.weight', 'decoder.layers.1.norm3.bias', 'fc_out.weight', 'fc_out.bias'])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\javer\\AppData\\Local\\Temp\\ipykernel_10056\\1285009507.py:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(\"transformer_translation1.pth\", map_location=device)\n"
     ]
    }
   ],
   "source": [
    "checkpoint = torch.load(\"transformer_translation1.pth\", map_location=device)\n",
    "print(checkpoint.keys())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformer(\n",
      "  (encoder): Encoder(\n",
      "    (embedding): Embedding(32000, 128)\n",
      "    (pos_encoding): PositionalEncoding()\n",
      "    (layers): ModuleList(\n",
      "      (0-1): 2 x EncoderLayer(\n",
      "        (self_attn): MultiHeadAttention(\n",
      "          (q_linear): Linear(in_features=128, out_features=128, bias=True)\n",
      "          (k_linear): Linear(in_features=128, out_features=128, bias=True)\n",
      "          (v_linear): Linear(in_features=128, out_features=128, bias=True)\n",
      "          (out): Linear(in_features=128, out_features=128, bias=True)\n",
      "        )\n",
      "        (ff): FeedForward(\n",
      "          (linear1): Linear(in_features=128, out_features=512, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (linear2): Linear(in_features=512, out_features=128, bias=True)\n",
      "        )\n",
      "        (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (decoder): Decoder(\n",
      "    (embedding): Embedding(32000, 128)\n",
      "    (pos_encoding): PositionalEncoding()\n",
      "    (layers): ModuleList(\n",
      "      (0-1): 2 x DecoderLayer(\n",
      "        (self_attn): MultiHeadAttention(\n",
      "          (q_linear): Linear(in_features=128, out_features=128, bias=True)\n",
      "          (k_linear): Linear(in_features=128, out_features=128, bias=True)\n",
      "          (v_linear): Linear(in_features=128, out_features=128, bias=True)\n",
      "          (out): Linear(in_features=128, out_features=128, bias=True)\n",
      "        )\n",
      "        (enc_dec_attn): MultiHeadAttention(\n",
      "          (q_linear): Linear(in_features=128, out_features=128, bias=True)\n",
      "          (k_linear): Linear(in_features=128, out_features=128, bias=True)\n",
      "          (v_linear): Linear(in_features=128, out_features=128, bias=True)\n",
      "          (out): Linear(in_features=128, out_features=128, bias=True)\n",
      "        )\n",
      "        (ff): FeedForward(\n",
      "          (linear1): Linear(in_features=128, out_features=512, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (linear2): Linear(in_features=512, out_features=128, bias=True)\n",
      "        )\n",
      "        (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm3): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (fc_out): Linear(in_features=128, out_features=32000, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translation: باتیں ہیں ہیں ہیں ہیں ہیں ہیں ہیں ہیں ہیں ہیں ہیں ہیں ہیں ہیں ہیں ہیں ہیں ہیں ہیں ہیں ہیں ہیں ہیں ہیں ہیں ہیں ہیں ہیں ہیں ہیں ہیں ہیں ہیں ہیں ہیں ہیں ہیں ہیں ہیں ہیں ہیں ہیں ہیں ہیں ہیں ہیں ہیں ہیں ہیں\n"
     ]
    }
   ],
   "source": [
    "def translate(model, tokenizer, sentence, max_len=50):\n",
    "    model.eval()\n",
    "    \n",
    "    # Tokenize the input English sentence\n",
    "    src_tokens = tokenizer.encode(sentence, out_type=int)\n",
    "    src_tokens = src_tokens[:max_len] + [PAD_IDX] * (max_len - len(src_tokens))  # Padding to max_len\n",
    "    src_input = torch.tensor(src_tokens).unsqueeze(0).to(device)  # Add batch dimension and move to device\n",
    "\n",
    "    # Generate translation using the model\n",
    "    with torch.no_grad():\n",
    "        output_tokens = model(src_input, src_input)  # Assuming the forward pass handles translation\n",
    "        \n",
    "        # If output_tokens is not in the expected format, e.g., logits, apply argmax\n",
    "        if isinstance(output_tokens, torch.Tensor):  # If it's a tensor\n",
    "            output_tokens = torch.argmax(output_tokens, dim=-1)  # Get the token IDs\n",
    "\n",
    "    # Decode token IDs to Urdu text\n",
    "    translation = tokenizer.decode(output_tokens.squeeze().tolist())  # Convert token IDs to text\n",
    "    return translation\n",
    "\n",
    "# Example usage\n",
    "english_sentence = \"In the name of Allah, the Most Gracious, the Most Merciful.\"\n",
    "urdu_translation = translate(model, tokenizer, english_sentence)\n",
    "print(\"Translation:\", urdu_translation)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=\"seagreen\"> **Transformer Model Evaluation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source: And to them it was given that they should not kill them , but that they should be tormented five months : and their torment was as the torment of a scorpion , when he striketh a man .\n",
      "Reference: اور انہیں جان سے مارنے کا نہیں بلکہ پانچ مہینے تک لوگوں کو اذیت دینے کا اختیار دیا گیا اور ان کی اذیت ایسی تھی جیسے بچھو کے ڈنک مارنے سے آدمی کو ہوتی ہے ۔\n",
      "Hypothesis: کو کو کو کو کو کو کو کو کو کو کو کو کو کو کو کو کو کو کو کو کو کو کو کو کو کو کو کو کو کو کو کو کو کو کو کو کو کو کو کو کو کو کو کو کو کو کو کو کو کو\n",
      "BLEU: 0.0000, ROUGE-1: 0.0000, ROUGE-L: 0.0000\n",
      "\n",
      "Source: And in those days shall men seek death , and shall not find it ; and shall desire to die , and death shall flee from them .\n",
      "Reference: ان دنوں میں آدمی موت ڈھونڈیں گے مگر ہرگز نہ پائیں گے اور مرنے کی آرزو کریں گے اور موت ان سے بھاگے گی ۔\n",
      "Hypothesis: ۔ ۔ ۔ ۔ ۔ ۔ ۔ ۔ ۔ ۔ ۔ ۔ ۔ ۔ ۔ ۔ ۔ ۔ ۔ ۔ ۔ ۔ ۔ ۔ ۔ ۔ ۔ ۔ ۔ ۔ ۔ ۔ ۔ ۔ ۔ ۔ ۔ ۔ ۔ ۔ ۔ ۔ ۔ ۔ ۔ ۔ ۔ ۔ ۔ ۔\n",
      "BLEU: 0.0000, ROUGE-1: 0.0000, ROUGE-L: 0.0000\n",
      "\n",
      "Source: And the shapes of the locusts were like unto horses prepared unto battle ; and on their heads were as it were crowns like gold , and their faces were as the faces of men .\n",
      "Reference: اور ان ڈڈیوں کی صورتیں ان گھوڑوں کی سی تھیں جو لڑائی کے لئے تیار کئے گئے ہوں اور ان کے سروں پر گویا سونے کے تاج تھے اور ان کے چہرے آدمیوں کے سے تھے ۔\n",
      "Hypothesis: کے کے کے کے کے کے کے کے کے کے کے کے کے کے کے کے کے کے کے کے کے کے کے کے کے کے کے کے کے کے کے کے کے کے کے کے کے کے کے کے کے کے کے کے کے کے کے کے کے کے\n",
      "BLEU: 0.0000, ROUGE-1: 0.0000, ROUGE-L: 0.0000\n",
      "\n",
      "Source: And they had hair as the hair of women , and their teeth were as the teeth of lions .\n",
      "Reference: اور بال عورتوں کے سے تھے اور دانت ببر کے سے ۔\n",
      "Hypothesis: ۔ ۔ ۔ ۔ ۔ ۔ ۔ ۔ ۔ ۔ ۔ ۔ ۔ ۔ ۔ ۔ ۔ ۔ ۔ ۔ ۔ ۔ ۔ ۔ ۔ ۔ ۔ ۔ ۔ ۔ ۔ ۔ ۔ ۔ ۔ ۔ ۔ ۔ ۔ ۔ ۔ ۔ ۔ ۔ ۔ ۔ ۔ ۔ ۔ ۔\n",
      "BLEU: 0.0000, ROUGE-1: 0.0000, ROUGE-L: 0.0000\n",
      "\n",
      "Source: And they had breastplates , as it were breastplates of iron ; and the sound of their wings was as the sound of chariots of many horses running to battle .\n",
      "Reference: ان کے پاس لوہے کے سے بکتر تھے اور ان کے پروں کی آواز ایسی تھی جیسے رتھوں اور بہت سے گھوڑوں کی جو لڑائی میں دوڑتے ہوں ۔\n",
      "Hypothesis: لوگوں لوگوں لوگوں لوگوں لوگوں لوگوں لوگوں لوگوں لوگوں لوگوں لوگوں لوگوں لوگوں لوگوں لوگوں لوگوں لوگوں لوگوں لوگوں لوگوں کے کے کے کے کے کے کے کے کے کے کے کے کے کے کے کے کے لوگوں لوگوں لوگوں لوگوں لوگوں لوگوں لوگوں لوگوں لوگوں لوگوں لوگوں لوگوں لوگوں\n",
      "BLEU: 0.0000, ROUGE-1: 0.0000, ROUGE-L: 0.0000\n",
      "\n",
      "Average BLEU Score: 0.0000\n",
      "Average ROUGE-1 Score: 0.0000\n",
      "Average ROUGE-L Score: 0.0000\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import nltk\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from rouge_score import rouge_scorer\n",
    "import sentencepiece as spm\n",
    "\n",
    "\n",
    "# Initialize the tokenizer\n",
    "class Tokenizer:\n",
    "    def __init__(self, model_path, max_seq_len=50):\n",
    "        self.tokenizer = spm.SentencePieceProcessor()\n",
    "        self.tokenizer.load(model_path)\n",
    "        self.MAX_SEQ_LEN = max_seq_len\n",
    "        self.PAD_IDX = self.tokenizer.piece_to_id(\"<pad>\")\n",
    "        self.SOS_IDX = self.tokenizer.piece_to_id(\"<sos>\")\n",
    "        self.EOS_IDX = self.tokenizer.piece_to_id(\"<eos>\")\n",
    "\n",
    "    def tokenize_and_prepare(self, sentences):\n",
    "        tokenized = [self.tokenizer.encode(sentence, out_type=int) for sentence in sentences]\n",
    "        padded = [seq[:self.MAX_SEQ_LEN] + [self.PAD_IDX] * (self.MAX_SEQ_LEN - len(seq)) \n",
    "                  if len(seq) < self.MAX_SEQ_LEN else seq[:self.MAX_SEQ_LEN] for seq in tokenized]\n",
    "        return padded\n",
    "\n",
    "    def decode_tokens(self, tokens):\n",
    "        return self.tokenizer.decode(tokens)\n",
    "\n",
    "\n",
    "# Load test data\n",
    "def load_test_data(en_file, ur_file):\n",
    "    with open(en_file, 'r', encoding='utf-8') as f:\n",
    "        source_sentences = f.readlines()\n",
    "    with open(ur_file, 'r', encoding='utf-8') as f:\n",
    "        reference_sentences = f.readlines()\n",
    "    return source_sentences, reference_sentences\n",
    "\n",
    "\n",
    "# BLEU score calculation\n",
    "def calculate_bleu(reference, hypothesis):\n",
    "    reference = [nltk.word_tokenize(reference.strip())]\n",
    "    hypothesis = nltk.word_tokenize(hypothesis.strip())\n",
    "    return sentence_bleu(reference, hypothesis)\n",
    "\n",
    "\n",
    "# ROUGE score calculation\n",
    "def calculate_rouge(reference, hypothesis):\n",
    "    scorer = rouge_scorer.RougeScorer(['rouge1', 'rougeL'], use_stemmer=True)\n",
    "    return scorer.score(reference.strip(), hypothesis.strip())\n",
    "\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "def evaluate_model(model, tokenizer, en_file, ur_file, device):\n",
    "    model.eval()\n",
    "    source_sentences, reference_sentences = load_test_data(en_file, ur_file)\n",
    "\n",
    "    total_bleu = 0\n",
    "    total_rouge1 = 0\n",
    "    total_rougeL = 0\n",
    "    scorer = rouge_scorer.RougeScorer(['rouge1', 'rougeL'], use_stemmer=True)\n",
    "\n",
    "    for i, (source, reference) in enumerate(zip(source_sentences, reference_sentences)):\n",
    "        # Tokenize and prepare input\n",
    "        source_tensor = torch.tensor(tokenizer.tokenize_and_prepare([source]), dtype=torch.long).to(device)\n",
    "        tgt_tensor = torch.full((source_tensor.size(0), tokenizer.MAX_SEQ_LEN), tokenizer.SOS_IDX, dtype=torch.long).to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            # Get model prediction\n",
    "            output = model(source_tensor, tgt_tensor)\n",
    "            prediction_ids = output.argmax(dim=-1).squeeze().tolist()\n",
    "            hypothesis = tokenizer.decode_tokens(prediction_ids)\n",
    "\n",
    "        # Calculate BLEU score\n",
    "        bleu_score = calculate_bleu(reference, hypothesis)\n",
    "        total_bleu += bleu_score\n",
    "\n",
    "        # Calculate ROUGE scores\n",
    "        rouge_scores = calculate_rouge(reference, hypothesis)\n",
    "        total_rouge1 += rouge_scores['rouge1'].fmeasure\n",
    "        total_rougeL += rouge_scores['rougeL'].fmeasure\n",
    "\n",
    "        # Print example predictions for the first 5\n",
    "        if i < 5:\n",
    "            print(f\"Source: {source.strip()}\")\n",
    "            print(f\"Reference: {reference.strip()}\")\n",
    "            print(f\"Hypothesis: {hypothesis.strip()}\")\n",
    "            print(f\"BLEU: {bleu_score:.4f}, ROUGE-1: {rouge_scores['rouge1'].fmeasure:.4f}, ROUGE-L: {rouge_scores['rougeL'].fmeasure:.4f}\\n\")\n",
    "\n",
    "    # Calculate and print average scores\n",
    "    num_samples = len(source_sentences)\n",
    "    avg_bleu = total_bleu / num_samples\n",
    "    avg_rouge1 = total_rouge1 / num_samples\n",
    "    avg_rougeL = total_rougeL / num_samples\n",
    "\n",
    "    print(f\"Average BLEU Score: {avg_bleu:.4f}\")\n",
    "    print(f\"Average ROUGE-1 Score: {avg_rouge1:.4f}\")\n",
    "    print(f\"Average ROUGE-L Score: {avg_rougeL:.4f}\")\n",
    "\n",
    "    return avg_bleu, avg_rouge1, avg_rougeL\n",
    "\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    en_file = \"umc005-corpus/bible/test.en\"\n",
    "    ur_file = \"umc005-corpus/bible/test.ur\"\n",
    "\n",
    "    tokenizer = Tokenizer(model_path=\"urdu_english_bpe_parallel.model\")\n",
    "    \n",
    "    # Assuming the trained model is loaded as `model`\n",
    "    avg_bleu, avg_rouge1, avg_rougeL = evaluate_model(model, tokenizer, en_file, ur_file, device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=\"seagreen\"> **Comparative Analysis: Transformer vs LSTM Model** TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=\"seagreen\"> **Loss Curve**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA04AAAIhCAYAAAB5deq6AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAABsKklEQVR4nO3deZyNdf/H8feZxSzM2JkZZizZyb6Nfd8iRbeKRCp3aJGkqNCNUndFUrTIkmRJ9VN2MVQIWROTspZBiMGYcZjr98f3ntGYMTPGONeZM6/n43E9Ouc613Xmc77O7Z637/f6XA7LsiwBAAAAAK7Ly+4CAAAAAMDdEZwAAAAAIAMEJwAAAADIAMEJAAAAADJAcAIAAACADBCcAAAAACADBCcAAAAAyADBCQAAAAAyQHACAAAAgAwQnAAgh3I4HJnaoqKiburnjB49Wg6HI0vnRkVFZUsNN/OzP//8c5f/7KzYuXOnHnroIZUpU0b+/v7Kly+fateurddff12nT5+2uzwAyPV87C4AAJA1GzZsSPF8zJgxWrNmjVavXp1if5UqVW7q5zzyyCPq0KFDls6tXbu2NmzYcNM1eLoPP/xQAwcOVMWKFfXss8+qSpUqcjqd2rJli6ZOnaoNGzboyy+/tLtMAMjVCE4AkEM1bNgwxfOiRYvKy8sr1f5rxcXFKTAwMNM/p2TJkipZsmSWagwODs6wntxuw4YNGjBggNq2bauvvvpKfn5+ya+1bdtWzzzzjJYtW5YtP+vixYvy9/fP8gwiAORmLNUDAA/WokULVatWTevWrVOjRo0UGBiofv36SZLmzZundu3aKTQ0VAEBAapcubKef/55XbhwIcV7pLVUr3Tp0urcubOWLVum2rVrKyAgQJUqVdLHH3+c4ri0lur17dtX+fLl02+//aZOnTopX758Cg8P1zPPPKOEhIQU5//xxx+65557FBQUpAIFCqhXr17avHmzHA6HZsyYkS1j9PPPP6tr164qWLCg/P39VbNmTc2cOTPFMYmJiRo7dqwqVqyogIAAFShQQNWrV9fbb7+dfMxff/2l/v37Kzw8XH5+fipatKgaN26sVatWpfvzX3nlFTkcDn3wwQcpQlOSPHny6M4770x+7nA4NHr06FTHlS5dWn379k1+PmPGDDkcDq1YsUL9+vVT0aJFFRgYqHnz5snhcOjbb79N9R5TpkyRw+HQzp07k/dt2bJFd955pwoVKiR/f3/VqlVL8+fPT/czAYAnYsYJADxcTEyMHnjgAQ0bNkyvvPKKvLzMv5nt27dPnTp10uDBg5U3b17t3btXr732mjZt2pRquV9aduzYoWeeeUbPP/+8ihcvro8++kgPP/ywypUrp2bNmqV7rtPp1J133qmHH35YzzzzjNatW6cxY8Yof/78GjlypCTpwoULatmypU6fPq3XXntN5cqV07Jly3Tvvffe/KD8T3R0tBo1aqRixYpp0qRJKly4sGbPnq2+ffvq+PHjGjZsmCTp9ddf1+jRo/Xiiy+qWbNmcjqd2rt3r86cOZP8Xr1799bWrVs1btw4VahQQWfOnNHWrVt16tSp6/78K1euaPXq1apTp47Cw8Oz7XP9U79+/XTHHXfok08+0YULF9S5c2cVK1ZM06dPV+vWrVMcO2PGDNWuXVvVq1eXJK1Zs0YdOnRQgwYNNHXqVOXPn19z587Vvffeq7i4uBRBDQA8ngUA8Ah9+vSx8ubNm2Jf8+bNLUnWt99+m+65iYmJltPptNauXWtJsnbs2JH82qhRo6xr/++iVKlSlr+/v3Xo0KHkfRcvXrQKFSpk/fvf/07et2bNGkuStWbNmhR1SrLmz5+f4j07depkVaxYMfn5u+++a0myli5dmuK4f//735Yka/r06el+pqSfvWDBgusec99991l+fn7W4cOHU+zv2LGjFRgYaJ05c8ayLMvq3LmzVbNmzXR/Xr58+azBgwene8y1jh07Zkmy7rvvvkyfI8kaNWpUqv2lSpWy+vTpk/x8+vTpliTrwQcfTHXskCFDrICAgOTPZ1mW9csvv1iSrHfeeSd5X6VKlaxatWpZTqczxfmdO3e2QkNDrStXrmS6bgDI6ViqBwAermDBgmrVqlWq/fv371fPnj0VEhIib29v+fr6qnnz5pKkPXv2ZPi+NWvWVERERPJzf39/VahQQYcOHcrwXIfDoS5duqTYV7169RTnrl27VkFBQakaU9x///0Zvn9mrV69Wq1bt04129O3b1/FxcUlN+CoX7++duzYoYEDB2r58uWKjY1N9V7169fXjBkzNHbsWG3cuFFOpzPb6rwZ3bt3T7WvX79+unjxoubNm5e8b/r06fLz81PPnj0lSb/99pv27t2rXr16SZIuX76cvHXq1EkxMTGKjo52zYcAADdAcAIADxcaGppq3/nz59W0aVP9+OOPGjt2rKKiorR582Z98cUXkkwTgYwULlw41T4/P79MnRsYGCh/f/9U58bHxyc/P3XqlIoXL57q3LT2ZdWpU6fSHJ+wsLDk1yVp+PDheuONN7Rx40Z17NhRhQsXVuvWrbVly5bkc+bNm6c+ffroo48+UmRkpAoVKqQHH3xQx44du+7PL1KkiAIDA3XgwIFs+0zXSuvzVa1aVfXq1dP06dMlmSWDs2fPVteuXVWoUCFJ0vHjxyVJQ4cOla+vb4pt4MCBkqSTJ0/esroBwN1wjRMAeLi0OqitXr1aR48eVVRUVPIsk6QU1+zYrXDhwtq0aVOq/ekFkaz8jJiYmFT7jx49KskEG0ny8fHRkCFDNGTIEJ05c0arVq3SiBEj1L59ex05ckSBgYEqUqSIJk6cqIkTJ+rw4cNatGiRnn/+eZ04ceK6XfG8vb3VunVrLV26VH/88Uemuhf6+fmlaqIh6brXUl2vg95DDz2kgQMHas+ePdq/f79iYmL00EMPJb+e9NmHDx+ubt26pfkeFStWzLBeAPAUzDgBQC6U9Mv0tV3c3n//fTvKSVPz5s117tw5LV26NMX+uXPnZtvPaN26dXKI/KdZs2YpMDAwzVbqBQoU0D333KNBgwbp9OnTOnjwYKpjIiIi9Pjjj6tt27baunVrujUMHz5clmXp0Ucf1aVLl1K97nQ69fXXXyc/L126dIqud5IJwufPn0/351zr/vvvl7+/v2bMmKEZM2aoRIkSateuXfLrFStWVPny5bVjxw7VrVs3zS0oKOiGfiYA5GTMOAFALtSoUSMVLFhQjz32mEaNGiVfX199+umn2rFjh92lJevTp48mTJigBx54QGPHjlW5cuW0dOlSLV++XJKSuwNmZOPGjWnub968uUaNGqVvvvlGLVu21MiRI1WoUCF9+umnWrx4sV5//XXlz59fktSlSxdVq1ZNdevWVdGiRXXo0CFNnDhRpUqVUvny5XX27Fm1bNlSPXv2VKVKlRQUFKTNmzdr2bJl152tSRIZGakpU6Zo4MCBqlOnjgYMGKCqVavK6XRq27Zt+uCDD1StWrXka8J69+6tl156SSNHjlTz5s31yy+/aPLkycm1ZlaBAgV09913a8aMGTpz5oyGDh2aakzff/99dezYUe3bt1ffvn1VokQJnT59Wnv27NHWrVu1YMGCG/qZAJCTEZwAIBcqXLiwFi9erGeeeUYPPPCA8ubNq65du2revHmqXbu23eVJkvLmzavVq1dr8ODBGjZsmBwOh9q1a6f33ntPnTp1UoECBTL1Pm+++Waa+9esWaMWLVpo/fr1GjFihAYNGqSLFy+qcuXKmj59eopW2y1bttTChQv10UcfKTY2ViEhIWrbtq1eeukl+fr6yt/fXw0aNNAnn3yigwcPyul0KiIiQs8991xyS/P0PProo6pfv74mTJig1157TceOHZOvr68qVKignj176vHHH08+9tlnn1VsbKxmzJihN954Q/Xr19f8+fPVtWvXTI3HPz300EP67LPPJCnN1uItW7bUpk2bNG7cOA0ePFh///23ChcurCpVqqhHjx43/PMAICdzWJZl2V0EAACZ9corr+jFF1/U4cOHM3VNEAAA2YEZJwCA25o8ebIkqVKlSnI6nVq9erUmTZqkBx54gNAEAHApghMAwG0FBgZqwoQJOnjwoBISEpKXv7344ot2lwYAyGVYqgcAAAAAGaAdOQAAAABkgOAEAAAAABkgOAEAAABABnJdc4jExEQdPXpUQUFBcjgcdpcDAAAAwCaWZencuXMKCwvL8MbquS44HT16VOHh4XaXAQAAAMBNHDlyJMPbXOS64BQUFCTJDE5wcLDN1Xg2p9OpFStWqF27dvL19bW7nFyBMXc9xty1GG/XY8xdjzF3Lcbb9dxpzGNjYxUeHp6cEdKT64JT0vK84OBggtMt5nQ6FRgYqODgYNv/R5FbMOaux5i7FuPteoy56zHmrsV4u547jnlmLuGhOQQAAAAAZIDgBAAAAAAZIDgBAAAAQAZy3TVOAAAA8BxXrlyR0+nM8vlOp1M+Pj6Kj4/XlStXsrEyXI+rx9zX11fe3t43/T4EJwAAAORI58+f1x9//CHLsrL8HpZlKSQkREeOHOEeny7i6jF3OBwqWbKk8uXLd1PvQ3ACAABAjnPlyhX98ccfCgwMVNGiRbP8C3hiYqLOnz+vfPnyZXgDVGQPV465ZVn666+/9Mcff6h8+fI3NfNEcAIAAECO43Q6ZVmWihYtqoCAgCy/T2Jioi5duiR/f3+Ck4u4esyLFi2qgwcPyul03lRw4tsBAACAHIvldchIdn1HCE4AAAAAkAGCEwAAAABkgOAEAAAA5GAtWrTQ4MGDM338wYMH5XA4tH379ltWkyciOAEAAAAu4HA40t369u2bpff94osvNGbMmEwfHx4erpiYGFWrVi1LPy+zPC2g0VUPAAAAcIGYmJjkx/PmzdPIkSMVHR2dvO/a7oBOp1O+vr4Zvm+hQoVuqA5vb2+FhITc0DlgxgkAAAAewLKkCxfs2TJ7/92QkJDkLX/+/HI4HMnP4+PjVaBAAc2fP18tWrSQv7+/Zs+erVOnTun+++9XyZIlFRgYqNtvv12fffZZive9dqle6dKl9corr6hfv34KCgpSRESEPvjgg+TXr50JioqKksPh0Lfffqu6desqMDBQjRo1ShHqJGns2LEqVqyYgoKC9Mgjj+j5559XzZo1s/LHJUlKSEjQk08+qWLFisnf319NmjTR5s2bk1//+++/1atXr+SW8+XLl9f06dMlSZcuXdLjjz+u0NBQ+fv7q3Tp0nr11VezXEtmEJwAAACQ48XFSfny3fgWHOylkiULKDjYK0vn58tnfnZ2ee655/Tkk09qz549at++veLj41WnTh198803+vnnn9W/f3/17t1bP/74Y7rv8+abb6pu3bratm2bBg4cqAEDBmjv3r3pnvPCCy/ozTff1JYtW+Tj46N+/folv/bpp59q3Lhxeu211/TTTz8pIiJCU6ZMuenPunDhQs2cOVNbt25VuXLl1L59e50+fVqS9NJLL+mXX37R0qVLtWfPHk2ZMkVFihSRJE2aNEmLFi3S/PnzFR0drdmzZ6t06dI3VU9GWKoHAAAAuInBgwerW7duKfYNHTo0+fETTzyhZcuWacGCBWrQoMF136dTp04aOHCgJBNQJkyYoKioKFWqVOm654wbN07NmzeXJD3//PO64447FB8fL39/f73zzjt6+OGH9dBDD0mSRo4cqRUrVuj8+fNZ+pwXLlzQ1KlTNWPGDHXs2FGS9OGHH2rlypWaNm2ann32WR0+fFi1atVS3bp1JSlFMDp8+LDKly+vJk2ayOFwqFSpUlmq40YQnGz099/S6tVSaKjUqJHd1QAAAORcgYFSVn6HT0xMVGxsrIKDg+XllbXFWIGBWTotTUkhIcmVK1c0fvx4zZs3T3/++acSEhKUkJCgvHnzpvs+1atXT36ctCTwxIkTmT4nNDRUknTixAlFREQoOjo6OYglqV+/vlavXp2pz3WtAwcOyOl0qnHjxsn7fH19Vb9+fe3Zs0eSNGDAAHXv3l1bt25Vu3btdNddd6nR/35p7tu3r9q2bauKFSuqQ4cO6ty5s9q1a5elWjKLpXo2eust6Z57pMmT7a4EAAAgZ3M4pLx57dkcjuz7HNcGojfffFMTJkzQsGHDtHr1am3fvl3t27fXpUuX0n2fa5tKOBwOJSYmZvocx/8+1D/PcVzzQa3MXtyVhqRz03rPpH0dO3bUoUOHNHjwYB09elStW7dOnn2rXbu2Dhw4oDFjxujixYvq0aOH7rnnnizXkxkEJxu1bWv+u3KllMH3GAAAALnQd999p65du+qBBx5QjRo1VLZsWe3bt8/ldVSsWFGbNm1KsW/Lli1Zfr+yZcsqT548+v7775P3OZ1ObdmyRZUrV07eV7RoUfXt21ezZ8/WxIkTUzS5CA4O1r333qsPP/xQ8+bN08KFC5Ovj7oVWKpno4YNzQWFJ09KO3ZItWrZXREAAADcSbly5bRw4UKtX79eBQsW1FtvvaVjx46lCBeu8MQTT+jRRx9V3bp11ahRI82bN087d+5U2bJlMzz32u58iYmJKlmypB577DE9++yzKlSokCIiIvT6668rLi5ODz/8sCRzHVWdOnVUtWpVJSQk6Jtvvkn+3BMmTFBoaKhq1qwpLy8vLViwQCEhISpQoEC2f/YkBCcb5ckjtWwpff21tGIFwQkAAAApvfTSSzpw4IDat2+vwMBA9e/fX3fddZfOnj3r0jp69eql/fv3a+jQoYqPj1ePHj3Ut2/fVLNQabnvvvtS7duxY4deffVVWZal3r1769y5c6pbt66WL1+uggULSpLy5Mmj4cOH6+DBgwoICFDTpk01d+5cSVK+fPn02muvad++ffL29la9evW0ZMmSLF+nlhkO62YWJ+ZAsbGxyp8/v86ePavg4GC7y9E770hPPim1aiV9+63d1WQvp9OpJUuWqFOnTpm6eRtuHmPueoy5azHerseYux5jnjnx8fE6cOCAypQpI39//yy/T3Y0h8it2rZtq5CQEH3yySc3dJ6rxzy978qNZANmnGyW1Pzj++/NPQCysysLAAAAkB3i4uI0depUtW/fXt7e3vrss8+0atUqrVy50u7SXIZYbbMKFaSICOnSJWndOrurAQAAAFJzOBxasmSJmjZtqjp16ujrr7/WwoUL1aZNG7tLcxlmnGzmcJjuetOmmeucOnSwuyIAAAAgpYCAAK1atcruMmzFjJMbSFqul4tmOgEAAIAcheDkBlq3NjNPP/8sHT1qdzUAAAA5Ry7rc4YsyK7vCMHJDRQuLNWpYx4z6wQAAJAxb29vSdKlS5dsrgTuLuk7kvSdySqucXIT7dpJW7aY4NSnj93VAAAAuDcfHx8FBgbqr7/+kq+vb5bbWicmJurSpUuKj4+nHbmLuHLMExMT9ddffykwMFA+PjcXfQhObqJdO+mVV0xwSkyU+N8tAADA9TkcDoWGhurAgQM6dOhQlt/HsixdvHhRAQEBcjgc2VghrsfVY+7l5aWIiIib/lkEJzcRGSnlzSudOCHt3CnVrGl3RQAAAO4tT548Kl++/E0t13M6nVq3bp2aNWvGDYddxNVjnidPnmyZ2SI4uYk8eaQWLaTFi82sE8EJAAAgY15eXvL398/y+d7e3rp8+bL8/f0JTi6SU8ecBWFuJKkt+YoV9tYBAAAAICWCkxtp29b897vvpIsX7a0FAAAAwFUEJzdSqZJUsqSUkGDCEwAAAAD3QHByIw4Hy/UAAAAAd0RwcjNJy/UITgAAAID7IDi5mTZtzMzTrl1STIzd1QAAAACQCE5up0gRqXZt83jVKntrAQAAAGAQnNwQy/UAAAAA90JwckNJDSJWrpQsy95aAAAAABCc3FKjRlJgoHT8uLnWCQAAAIC9CE5uyM9Pat7cPGa5HgAAAGA/gpOb+udyPQAAAAD2Iji5qaTgtG6ddPGivbUAAAAAuR3ByU1VriyFhUnx8dL339tdDQAAAJC7EZzclMPBcj0AAADAXRCc3FhScKJBBAAAAGAvgpMba93a/HfHDtOaHAAAAIA9CE5urFgxqVYt83jVKntrAQAAAHIzgpObY7keAAAAYD+Ck5tr29b8d+VKybLsrQUAAADIrQhObq5xYykgQIqJkXbvtrsaAAAAIHciOLk5f3+peXPzmOV6AAAAgD0ITjlA0nI9ghMAAABgD4JTDpDUIGLdOik+3t5aAAAAgNyI4JQDVK0qhYZKFy9KP/xgdzUAAABA7kNwygEcDpbrAQAAAHYiOOUQScv1Vq60tw4AAAAgNyI45RBt2pj/btsmnThhby0AAABAbkNwyiGKF5dq1DCPV62ytxYAAAAgtyE45SAs1wMAAADsQXDKQZKC04oVkmXZWwsAAACQmxCccpAmTSR/f+noUemXX+yuBgAAAMg9CE45iL+/1KyZecxyPQAAAMB1bA1Oo0ePlsPhSLGFhIRc9/ioqKhUxzscDu3du9eFVdvrn8v1AAAAALiGj90FVK1aVav+0SbO29s7w3Oio6MVHByc/Lxo0aK3pDZ3lHQj3KgoKSFB8vOztRwAAAAgV7A9OPn4+KQ7y5SWYsWKqUCBAremIDd3++2mNfnx49L69VLLlnZXBAAAAHg+24PTvn37FBYWJj8/PzVo0ECvvPKKypYtm+45tWrVUnx8vKpUqaIXX3xRLdNJDwkJCUpISEh+HhsbK0lyOp1yOp3Z8yFcrE0bb336qZeWLr2iJk0S7S7nupLGN6eOc07EmLseY+5ajLfrMeaux5i7FuPteu405jdSg8Oy7GtsvXTpUsXFxalChQo6fvy4xo4dq71792r37t0qXLhwquOjo6O1bt061alTRwkJCfrkk080depURUVFqVlS14RrjB49Wi+//HKq/XPmzFFgYGC2fyZXWLOmpN5+u47Klj2jt95aa3c5AAAAQI4UFxennj176uzZsykuBUqLrcHpWhcuXNBtt92mYcOGaciQIZk6p0uXLnI4HFq0aFGar6c14xQeHq6TJ09mODjuKiZGKlXKVw6HpT/+uCx3vcTL6XRq5cqVatu2rXx9fe0uJ1dgzF2PMXctxtv1GHPXY8xdi/F2PXca89jYWBUpUiRTwcn2pXr/lDdvXt1+++3at29fps9p2LChZs+efd3X/fz85JdGBwVfX1/b/6CyKiJCql5d2rnToXXrfHXffXZXlL6cPNY5FWPueoy5azHerseYux5j7lqMt+u5w5jfyM93q/s4JSQkaM+ePQoNDc30Odu2bbuh4z1FUnc97ucEAAAA3Hq2zjgNHTpUXbp0UUREhE6cOKGxY8cqNjZWffr0kSQNHz5cf/75p2bNmiVJmjhxokqXLq2qVavq0qVLmj17thYuXKiFCxfa+TFs0a6d9Oab5n5OliU5HHZXBAAAAHguW4PTH3/8ofvvv18nT55U0aJF1bBhQ23cuFGlSpWSJMXExOjw4cPJx1+6dElDhw7Vn3/+qYCAAFWtWlWLFy9Wp06d7PoItmna1NzD6Y8/pL17pcqV7a4IAAAA8Fy2Bqe5c+em+/qMGTNSPB82bJiGDRt2CyvKOQICTHhatcos1yM4AQAAALeOW13jhBvTrp3574oV9tYBAAAAeDqCUw6WFJyioqRLl2wtBQAAAPBoBKcc7PbbpWLFpAsXpA0b7K4GAAAA8FwEpxzMy+tqW3KW6wEAAAC3DsEph+M6JwAAAODWIzjlcG3amP/+9JN06pS9tQAAAACeiuCUw4WFSdWqmZvgfvut3dUAAAAAnong5AFYrgcAAADcWgQnD5DUIGLlSjPzBAAAACB7EZw8QLNmUp480uHD0q+/2l0NAAAA4HkITh4gMFBq2tQ8ZrkeAAAAkP0ITh7in8v1AAAAAGQvgpOHSGoQsWaNdOmSvbUAAAAAnobg5CFq1JCKFpXOn5c2brS7GgAAAMCzEJw8hJfX1ZvhslwPAAAAyF4EJw/C/ZwAAACAW4Pg5EGSGkRs3iydPm1vLQAAAIAnITh5kBIlpCpVzE1wV6+2uxoAAADAcxCcPAzL9QAAAIDsR3DyMP8MTpZlby0AAACApyA4eZhmzaQ8eaRDh6TffrO7GgAAAMAzEJw8TN68UuPG5jHL9QAAAIDsQXDyQFznBAAAAGQvgpMHSmpLvmaN5HTaWwsAAADgCQhOHqhWLalwYencOenHH+2uBgAAAMj5CE4eyMvr6qwTy/UAAACAm0dw8lBJwWnlSnvrAAAAADwBwclDJQWnTZukv/+2txYAAAAgpyM4eajwcKlyZSkxUVq92u5qAAAAgJyN4OTBWK4HAAAAZA+CkwdLup/T8uWSZdlbCwAAAJCTEZw8WPPmkq+vdPCg9PvvdlcDAAAA5FwEJw+WL5/UqJF5zHI9AAAAIOsITh4uabke93MCAAAAso7g5OGSgtPq1dLly/bWAgAAAORUBCcPV6uWVKiQFBtr7ukEAAAA4MYRnDyct7fUpo15zHI9AAAAIGsITrkA1zkBAAAAN4fglAsk3Qh30ybpzBlbSwEAAAByJIJTLhARIVWsKF25Iq1ZY3c1AAAAQM5DcMolWK4HAAAAZB3BKZdIWq7HjXABAACAG0dwyiVatJB8fKTffzcbAAAAgMwjOOUSQUFSo0bmMbNOAAAAwI0hOOUiLNcDAAAAsobglIskNYj49lvp8mV7awEAAAByEoJTLlKnjlSwoHT2rLR5s93VAAAAADkHwSkX8faWWrc2j1muBwAAAGQewSmX4X5OAAAAwI0jOOUySQ0iNm40S/YAAAAAZIzglMuULi2VLy9duSJFRdldDQAAAJAzEJxyIZbrAQAAADeG4JQLEZwAAACAG0NwyoVatDAd9n77TTpwwO5qAAAAAPdHcMqFgoOlyEjzmLbkAAAAQMYITrlU0nI9ghMAAACQMYJTLpXUlnzVKtNhDwAAAMD1EZxyqbp1pQIFpDNnpC1b7K4GAAAAcG8Ep1zKx0dq3do8ZrkeAAAAkD6CUy6WtFyPtuQAAABA+ghOuVhSg4gNG6Rz5+ytBQAAAHBnBKdcrEwZqVw56fJlKSrK7moAAAAA92VrcBo9erQcDkeKLSQkJN1z1q5dqzp16sjf319ly5bV1KlTXVStZ2K5HgAAAJAx22ecqlatqpiYmORt165d1z32wIED6tSpk5o2bapt27ZpxIgRevLJJ7Vw4UIXVuxZkpbrEZwAAACA6/OxvQAfnwxnmZJMnTpVERERmjhxoiSpcuXK2rJli9544w117979FlbpuVq2lLy9pV9/lQ4dkkqVsrsiAAAAwP3YHpz27dunsLAw+fn5qUGDBnrllVdUtmzZNI/dsGGD2iVNkfxP+/btNW3aNDmdTvn6+qY6JyEhQQkJCcnPY2NjJUlOp1NOpzMbP0nOFBgo1a/vrQ0bvLRs2WX162dl23snjS/j7DqMuesx5q7FeLseY+56jLlrMd6u505jfiM1OCzLyr7flG/Q0qVLFRcXpwoVKuj48eMaO3as9u7dq927d6tw4cKpjq9QoYL69u2rESNGJO9bv369GjdurKNHjyo0NDTVOaNHj9bLL7+cav+cOXMUGBiYvR8oh5o7t6Lmzq2kRo3+1LBh3A0XAAAAuUNcXJx69uyps2fPKjg4ON1jbQ1O17pw4YJuu+02DRs2TEOGDEn1eoUKFfTQQw9p+PDhyft++OEHNWnSRDExMWku+Utrxik8PFwnT57McHByi40bHWrWzEeFCln688/L8vbOnvd1Op1auXKl2rZtm+ZsILIfY+56jLlrMd6ux5i7HmPuWoy367nTmMfGxqpIkSKZCk62L9X7p7x58+r222/Xvn370nw9JCREx44dS7HvxIkT8vHxSXOGSpL8/Pzk5+eXar+vr6/tf1DuIjJSyp9fOn3aoV27fFWvXva+P2Pteoy56zHmrsV4ux5j7nqMuWsx3q7nDmN+Iz/f9q56/5SQkKA9e/akueROkiIjI7Vy5coU+1asWKG6devaPug5mY+P1KqVeUx3PQAAACA1W4PT0KFDtXbtWh04cEA//vij7rnnHsXGxqpPnz6SpOHDh+vBBx9MPv6xxx7ToUOHNGTIEO3Zs0cff/yxpk2bpqFDh9r1ETxGUs+Na3IpAAAAANm8VO+PP/7Q/fffr5MnT6po0aJq2LChNm7cqFL/64kdExOjw4cPJx9fpkwZLVmyRE8//bTeffddhYWFadKkSbQizwZJN8Jdv146d04KCrK3HgAAAMCd2Bqc5s6dm+7rM2bMSLWvefPm2rp16y2qKPe67TapbFlp/35p7Vqpc2e7KwIAAADch1td4wR7sVwPAAAASBvBCcmSluvRIAIAAABIieCEZK1aSV5e0t690pEjdlcDAAAAuA+CE5IVKCA1aGAes1wPAAAAuIrghBSSlut9/rlkWfbWAgAAALgLghNS+Ne/JG9vaelS6aOP7K4GAAAAcA8EJ6RQrZr06qvm8ZNPSjt32lsPAAAA4A4ITkjlmWekTp2k+HipRw/p/Hm7KwIAAADsRXBCKl5e0syZUsmSUnS0NGAA1zsBAAAgdyM4IU1Fikhz55rrnWbPlj7+2O6KAAAAAPsQnHBdjRtL48aZx48/Lu3aZW89AAAAgF0ITkjXs89KHTtyvRMAAAByN4IT0uXlJc2aJZUoIe3dKw0cyPVOAAAAyH0ITsjQP693+uQTacYMuysCAAAAXIvghExp0kQaM8Y8HjRI2r3b3noAAAAAVyI4IdOee05q3166eFH617+kCxfsrggAAABwDYITMs3LyyzVCwuT9uwxM08AAABAbkBwwg0pWlT67LOrN8nleicAAADkBgQn3LBmzaT//Mc8HjiQ650AAADg+QhOyJLhw6W2bc31Tj16cL0TAAAAPBvBCVni5SXNni2Fhkq//CI98YTdFQEAAAC3DsEJWVasmDRnjglR06ebG+UCAAAAnojghJvSooU0erR5PGCA6bYHAAAAeBqCE27aiBFSmzZSXJy5v1NcnN0VAQAAANmL4ISb5u1trncKCTEd9p580u6KAAAAgOxFcEK2KF786vVO06aZIAUAAAB4CoITsk3LltLIkebxY49Je/faWw8AAACQXQhOyFYvvii1amXu63T//T5KSPC2uyQAAADgphGckK28vaVPPzVL93bvduijj6rZXRIAAABw0whOyHYhISY8ORyWVq4srTlzHHaXBAAAANwUghNuidatpRdeSJQkDRrkrehomwsCAAAAbgLBCbfMCy8k6vbb/9KFCw716CFdvGh3RQAAAEDWEJxwy3h7S08//ZOKFbO0c6c0eLDdFQEAAABZQ3DCLVWoUIJmzrwih0P64APps8/srggAAAC4cQQn3HKtW1t68UXzuH9/6ddf7a0HAAAAuFEEJ7jEqFFS8+bS+fNSjx5SfLzdFQEAAACZR3CCS3h7S3PmSEWLSjt2SE8/bXdFAAAAQOYRnOAyYWHS7NmSwyFNnSrNm2d3RQAAAEDmEJzgUu3aSSNGmMePPirt22dvPQAAAEBmEJzgcqNHS02bSufOcb0TAAAAcgaCE1zOx8e0JS9SRNq+XXrmGbsrAgAAANJHcIItSpSQPvnEPH7vPWn+fHvrAQAAANJDcIJtOnSQhg83jx95RPrtN3vrAQAAAK6H4ARb/ec/UpMm5nqne++VEhLsrggAAABIjeAEWyVd71S4sLR1qzR0qN0VAQAAAKkRnGC7kiWvXu80ebL0+ef21gMAAABci+AEt9Cxo/Tcc+bxww9L+/fbWw8AAADwTwQnuI0xY6TGjaXYWHN/J653AgAAgLsgOMFt+Pqa650KFZJ++kl69lm7KwIAAAAMghPcSni4NGuWefzOO9IXX9hbDwAAACARnOCG7rjj6mxTv35c7wQAAAD7EZzglsaNkyIjpbNnpfvuky5dsrsiAAAA5GYEJ7glX19p7lypYEFp82Zp2DC7KwIAAEBuRnCC24qIkGbONI/ffpvrnQAAAGAfghPcWpcu0jPPmMf332+67gEAAACuRnCC23v1Vemee8x1Tj17Sq+9JlmW3VUBAAAgNyE4we35+krz5klPP22eP/+8NGiQdPmyvXUBAAAg9yA4IUfw8pLeekuaOFFyOKQpU6S775YuXLC7MgAAAOQGBCfkKE89JX3+ueTvL33zjdSypXT8uN1VAQAAwNMRnJDjdOsmrV4tFS5sWpVHRkrR0XZXBQAAAE9GcEKOFBkpbdgg3XabdOCA1KiR9P33dlcFAAAAT0VwQo5Vvry0fr3UoIF0+rTUpo20YIHdVQEAAMATuU1wevXVV+VwODR48ODrHhMVFSWHw5Fq27t3r+sKhVspVsws2+vaVUpIkO691zSRoF05AAAAspNbBKfNmzfrgw8+UPXq1TN1fHR0tGJiYpK38uXL3+IK4c4CA6WFC6XHHzeB6ZlnpMGDpStX7K4MAAAAnsL24HT+/Hn16tVLH374oQoWLJipc4oVK6aQkJDkzdvb+xZXCXfn7S1NmiS98YZ5PmmS9K9/SXFx9tYFAAAAz+BjdwGDBg3SHXfcoTZt2mjs2LGZOqdWrVqKj49XlSpV9OKLL6ply5bXPTYhIUEJCQnJz2NjYyVJTqdTTqfz5opHupLG15Xj/OSTUmioQw895K0vv3SoVatEffHFFRUt6rISbGXHmOd2jLlrMd6ux5i7HmPuWoy367nTmN9IDQ7Lsu9qkLlz52rcuHHavHmz/P391aJFC9WsWVMTJ05M8/jo6GitW7dOderUUUJCgj755BNNnTpVUVFRatasWZrnjB49Wi+//HKq/XPmzFFgYGB2fhy4kd27C+nVVxvo/Pk8Cg09r5EjNyo0lLvlAgAA4Kq4uDj17NlTZ8+eVXBwcLrHZik4HTlyRA6HQyVLlpQkbdq0SXPmzFGVKlXUv3//TL9H3bp1tWLFCtWoUUOSMgxOaenSpYscDocWLVqU5utpzTiFh4fr5MmTGQ4Obo7T6dTKlSvVtm1b+fr6uvzn790r3Xmnjw4edKhIEUtffnlFDRp4dtcIu8c8N2LMXYvxdj3G3PUYc9divF3PncY8NjZWRYoUyVRwytJSvZ49e6p///7q3bu3jh07prZt26pq1aqaPXu2jh07ppEjR2b4Hj/99JNOnDihOnXqJO+7cuWK1q1bp8mTJyshISFT1y41bNhQs2fPvu7rfn5+8vPzS7Xf19fX9j+o3MKusb79dmnjRqlzZ2nLFofatvXRnDnS3Xe7vBSX4/vteoy5azHerseYux5j7lqMt+u5w5jfyM/PUnOIn3/+WfXr15ckzZ8/X9WqVdP69es1Z84czZgxI1Pv0bp1a+3atUvbt29P3urWratevXpp+/btmW74sG3bNoWGhmblYyAXKF5cioqS7rhDio+XuneX3nnH7qoAAACQ02RpxsnpdCbP4qxatUp33nmnJKlSpUqKiYnJ1HsEBQWpWrVqKfblzZtXhQsXTt4/fPhw/fnnn5o1a5YkaeLEiSpdurSqVq2qS5cuafbs2Vq4cKEWLlyYlY+BXCJvXumrr0y78vffNw0kDh2SXn9d8rK9ryQAAABygiz92li1alVNnTpV3333nVauXKkOHTpIko4eParChQtnW3ExMTE6fPhw8vNLly5p6NChql69upo2barvv/9eixcvVrdu3bLtZ8Iz+fhIU6ZIr75qnr/5pnTffWYWCgAAAMhIlmacXnvtNd19993673//qz59+iQ3d1i0aFHyEr6siIqKSvH82mV/w4YN07Bhw7L8/sjdHA7p+eeliAipb19pwQLp6FHp//5Pysa8DwAAAA+UpeDUokULnTx5UrGxsSluWtu/f39afMPt9ewphYVJd90l/fCD1LixtHSpVKaM3ZUBAADAXWVpqd7FixeVkJCQHJoOHTqkiRMnKjo6WsWKFcvWAoFboUULE5rCw6XoaKlhQ2nLFrurAgAAgLvKUnDq2rVrcsOGM2fOqEGDBnrzzTd11113acqUKdlaIHCrVK1q2pXXrCmdOCE1by59843dVQEAAMAdZSk4bd26VU2bNpUkff755ypevLgOHTqkWbNmadKkSdlaIHArhYVJ69ZJ7dtLcXFS167S1Kl2VwUAAAB3k6XgFBcXp6CgIEnSihUr1K1bN3l5ealhw4Y6dOhQthYI3GpBQdLXX0sPPywlJkoDBkjDh5vHAAAAgJTF4FSuXDl99dVXOnLkiJYvX6527dpJkk6cOKHg4OBsLRBwBV9f6cMPpf/8xzwfP17q3VtKSLC3LgAAALiHLAWnkSNHaujQoSpdurTq16+vyMhISWb2qVatWtlaIOAqDof00kvSjBnmvk9z5pglfH//bXdlAAAAsFuWgtM999yjw4cPa8uWLVq+fHny/tatW2vChAnZVhxghz59THvyoCBp7VqpSROJFagAAAC5W5aCkySFhISoVq1aOnr0qP78809JUv369VWpUqVsKw6wS5s20vffSyVKSL/8YtqVb9tmd1UAAACwS5aCU2Jiov7zn/8of/78KlWqlCIiIlSgQAGNGTNGiVxRDw9RvbppV3777dKxY1KzZtKyZXZXBQAAADtkKTi98MILmjx5ssaPH69t27Zp69ateuWVV/TOO+/opZdeyu4aAduULCl9953UurV0/rzUubM0bZrdVQEAAMDVfLJy0syZM/XRRx/pzjvvTN5Xo0YNlShRQgMHDtS4ceOyrUDAbvnzS0uWSI8+Ks2aJT3yiLnm6eWXTUMJAAAAeL4szTidPn06zWuZKlWqpNOnT990UYC7yZPHdNsbOdI8HzNG6ttXunTJzqoAAADgKlkKTjVq1NDkyZNT7Z88ebKqV69+00UB7sjhMLNMH30keXub2afWraXff7e7MgAAANxqWVqq9/rrr+uOO+7QqlWrFBkZKYfDofXr1+vIkSNasmRJdtcIuJWHHzbXPt1zj+m8V726uWHuoEGSV5b7VAIAAMCdZenXvObNm+vXX3/V3XffrTNnzuj06dPq1q2bdu/erenTp2d3jYDbad9e2rFDatlSiouTnnxSatFC2rfP7soAAABwK2RpxkmSwsLCUjWB2LFjh2bOnKmPP/74pgsD3F3ZstKqVdIHH0jPPmu679WoIY0bZ4KUt7fdFQIAACC7sLAIuAleXtJjj0m7dpnrnS5elIYMMfd8io62uzoAAABkF4ITkA1Kl5ZWrpTef18KCpLWr5dq1pTeeEO6csXu6gAAAHCzCE5ANnE4pP79pZ9/ltq1k+LjzRK+Jk2kPXvsrg4AAAA344aucerWrVu6r585c+ZmagE8QkSEtGyZNH269PTT0saNUq1appX5M89IPlm+shAAAAB2uaEZp/z586e7lSpVSg8++OCtqhXIMRwOqV8/afduqVMnKSFBev55qVEjsw8AAAA5yw392zetxoEbU7Kk9M035ma5Tz0lbd4s1a4tjRwpDRsm+fraXSEAAAAyg2ucgFvM4ZD69JF++UXq3Fm6dEl68UWpYUNp5067qwMAAEBmEJwAFwkLkxYtkj75RCpYUNq6VapbV/rPfySn0+7qAAAAkB6CE+BCDof0wAPmOqe77jKBadQoqX59aft2u6sDAADA9RCcABuEhkpffCF99plUuLAJTfXqmRB16ZLd1QEAAOBaBCfAJg6HdN99Zvape3fp8mWzbK9ePbOMDwAAAO6D4ATYrHhx6fPPpfnzpSJFTMOI+vVNA4mEBLurAwAAgERwAtzGv/5lOu/16CFduSKNGyfVqWNamAMAAMBeBCfAjRQtKs2bZ2agihUzy/gaNpSGD5fi4+2uDgAAIPciOAFuqHt3E5p69pQSE6Xx482NczdutLsyAACA3IngBLipIkWkTz+VvvzSXAe1Z4/UuLH07LPSxYt2VwcAAJC7EJwAN3fXXebap969zezTG29INWtK69fbXRkAAEDuQXACcoBChaRZs6Svv5bCwqRff5WaNJGGDJHi4uyuDgAAwPMRnIAcpHNn6eefpb59JcuSJkyQatSQvvvO7soAAAA8G8EJyGEKFpSmT5eWLJFKlJB++01q3lx66inpwgW7qwMAAPBMBCcgh+rY0XTee/hhM/s0aZJUp46Pdu4sYndpAAAAHofgBORg+fNLH30kLVsmhYdL+/c7NHJkY3Xv7q1ff7W7OgAAAM9BcAI8QPv25tqnAQOuyMsrUV9/7aWqVaUnnpD++svu6gAAAHI+ghPgIYKDpbffTtTbb69Rp06JunxZmjxZKldOev11KT7e7goBAAByLoIT4GHCw8/rq6+u6NtvpVq1pNhY6bnnpIoVpTlzzL2gAAAAcGMIToCHatVK2rJFmjnTdN87fFjq1Utq2FBat87u6gAAAHIWghPgwby8pAcfNDfMHTdOypdP2rzZtC+/+27RQAIAACCTCE5ALhAYKI0YYe759Nhjkre39NVXSm4gcfKk3RUCAAC4N4ITkIsULy5NmSLt3Cl17qzkBhK33UYDCQAAgPQQnIBcqEoV6euvpVWrpJo1rzaQqFRJ+uwzGkgAAABci+AE5GKtW0s//XS1gcShQ1LPnqaBxHff2V0dAACA+yA4AbncPxtIjB17tYFEs2ZSt240kAAAAJAITgD+JzBQeuEF00Di3/82gerLL00DiSefpIEEAADI3QhOAFIoXlyaOlXatUu64w7TQOKdd6Ry5aT//pcGEgAAIHciOAFIU5Uq0jffmAYSNWpIZ89Kw4ZdbSBhWXZXCAAA4DoEJwDpSmogMWMGDSQAAEDuRXACkCFvb6lPn5QNJDZtutpAYt8+uysEAAC4tQhOADItqYHEvn0pG0hUqSI99RQNJAAAgOciOAG4YSEhpoHEzp1Sp06mgcSkSTSQAAAAnovgBCDLqlaVFi+WVq5M2UCicmVp7lwaSAAAAM9BcAJw09q0MQ0kpk+XwsKkgwel++83DSS+/97u6gAAAG4ewQlAtvD2lvr2Ndc/jRkj5c1rGkg0bSrdfbe0fbvdFQIAAGQdwQlAtgoMlF58UfrtN6l/f9NA4quvpFq1pK5dpS1b7K4QAADgxhGcANwSISHS++9Lu3aZZXteXtKiRVK9etIdd0gbN9pdIQAAQOYRnADcUlWqSHPmSL/8Ij34oFnSt2SJFBkptWvHNVAAACBncJvg9Oqrr8rhcGjw4MHpHrd27VrVqVNH/v7+Klu2rKZOneqaAgHclIoVpZkzpb17pX79JB8f042vaVOpVSspKooufAAAwH25RXDavHmzPvjgA1WvXj3d4w4cOKBOnTqpadOm2rZtm0aMGKEnn3xSCxcudFGlAG5WuXLStGnSr7+aa6B8faU1a6SWLaVmzUyYIkABAAB3Y3twOn/+vHr16qUPP/xQBQsWTPfYqVOnKiIiQhMnTlTlypX1yCOPqF+/fnrjjTdcVC2A7FKmjLkG6rffpEGDpDx5zLK9du2kRo2kpUsJUAAAwH342F3AoEGDdMcdd6hNmzYaO3Zsusdu2LBB7dq1S7Gvffv2mjZtmpxOp3x9fVOdk5CQoISEhOTnsbGxkiSn0ymn05kNnwDXkzS+jLPr5MQxDw2VJkyQhg6V3nrLSx9+6KWNGx3q1EmqUydRI0YkqnNnSw6H3ZWmLSeOeU7GeLseY+56jLlrMd6u505jfiM12Bqc5s6dq61bt2rz5s2ZOv7YsWMqXrx4in3FixfX5cuXdfLkSYWGhqY659VXX9XLL7+cav+KFSsUGBiYtcJxQ1auXGl3CblOTh3z1q2l2rX99NVX5bRsWWn99JOPunf3UpkyZ9Sjx69q0CBGXrbPk6ctp455TsV4ux5j7nqMuWsx3q7nDmMeFxeX6WNtC05HjhzRU089pRUrVsjf3z/T5zmu+Wdn639rea7dn2T48OEaMmRI8vPY2FiFh4erXbt2Cg4OzkLlyCyn06mVK1eqbdu2ac4GIvt5ypj36iX99ZeliROvaMoULx04UECvvVZfVataGj78irp3t+TtbXeVhqeMeU7BeLseY+56jLlrMd6u505jnrQaLTNsC04//fSTTpw4oTp16iTvu3LlitatW6fJkycrISFB3tf8ZhQSEqJjx46l2HfixAn5+PiocOHCaf4cPz8/+fn5pdrv6+tr+x9UbsFYu54njHlYmPT669Jzz0kTJ0qTJkm7dzv0wAM+GjvW3GT33ntNdz534AljnpMw3q7HmLseY+5ajLfrucOY38jPt23RS+vWrbVr1y5t3749eatbt6569eql7du3pwpNkhQZGZlqSm/FihWqW7eu7YMO4NYoXFgaM0Y6dEh6+WWpQAHT0vyBB8w9ombOlNxgiTQAAPBwtgWnoKAgVatWLcWWN29eFS5cWNWqVZNkltk9+OCDyec89thjOnTokIYMGaI9e/bo448/1rRp0zR06FC7PgYAFylQQBo50gSoceOkQoWkffukvn3NPaI++ki6dMnuKgEAgKdy08usjZiYGB0+fDj5eZkyZbRkyRJFRUWpZs2aGjNmjCZNmqTu3bvbWCUAVwoOlkaMkA4elF57TSpaVDpwQHr0Ual8eWnqVOkfjTQBAACyhZtcHWBERUWleD5jxoxUxzRv3lxbt251TUEA3FZQkDRsmLkH1PvvS//9r3T4sDRggDR2rLk26pFHpIAAuysFAACewK1nnAAgI3nzSkOGSPv3mwYSYWHSn39KTz4plS1r7hF1A51GAQAA0kRwAuARAgKkJ56Qfv9deu89KTxcOnbMhKoyZUyHvvPn7a4SAADkVAQnAB7F398s1/vtN+nDD01oOnHCLN0rXVp65RXpBm7ZAAAAIIngBMBD5cljrnGKjpamT5fKlZNOnZJeeEEqVUoaPVqKibG7SgAAkFMQnAB4NF9f07J8zx5p9mypUiXpzBlzT6iICHMT3bVrJcuyu1IAAODOCE4AcgUfH6lXL+nnn6W5c6VGjaTLl6X586UWLaRq1aR332UZHwAASBvBCUCu4u1tZpl++EHatk3q318KDJR++UV6/HHTlW/AAGnXLrsrBQAA7oTgBCDXqlnT3APq6FHTyrxSJenCBXMT3erVpaZNpc8+ky5dsrtSAABgN4ITgFwvf37TyvyXX6TVq6V77jEzU99/L/XsaVqbv/CCucEuAADInQhOAPA/DofUsqW0YIF06JDpvBcWZtqZv/KKaW3etau0fLmUmGh3tQAAwJUITgCQhhIlpFGjpIMHpc8/l1q1MmFp0SKpQwepYkXprbek06ftrhQAALgCwQkA0uHrK3XvLn37rVnK9+STUnCwucHuM89IpUv76J13auqnnxx2lwoAAG4hghMAZFLlytLbb5tmEh98INWoIcXHO/Ttt6UUGemj+vWlGTOkixftrhQAAGQ3ghMA3KC8eaVHHzXtzNetu6zmzY8oTx5LmzdLDz1klvkNHWpmpQAAgGcgOAFAFjkcUsOGlp5+eqsOHLis8eOlUqWkv/+W3nxTKl/eXA+1aJF05Yrd1QIAgJtBcAKAbFC0qPTcc9Lvv0vffCN16mSC1fLlphNf2bKmM9/x43ZXCgAAsoLgBADZyNtbuuMOafFis1Rv2DCpcGFzD6gXXjD3hOrZ09wjyrLsrhYAAGQWwQkAbpGyZaXXXpP++EOaOVNq0EByOqXPPpOaNjXNJaZOlc6ds7tSAACQEYITANxi/v7Sgw9KGzdKP/0kPfKIFBAg7dolDRhgmkk8/ri0e7fdlQIAgOshOAGAC9WuLX34ofTnn9LEiVKFCmbG6d13pWrVpMaNpY8/ls6ft7tSAADwTwQnALBBwYLSU09Je/dKq1ZJd99tro9av156+GEpNFTq31/atIlroQAAcAcEJwCwkcMhtW4tffGFdOSINH68VK6cmXH68ENzXVT16ubGu6dO2V0tAAC5F8EJANxEaKhpaf7rr1JUlNS7t7k+6uefpcGDpbAw6f77zQxVYqLd1QIAkLsQnADAzTgcUvPm0qxZUkyMuf6pdm3p0iVp7lypbVvpttuksWNNxz4AAHDrEZwAwI0VKCANHGi68f30k3mcP7908KD00ktSqVLmvlFffmlanQMAgFuD4AQAOUTt2mb26ehR6ZNPzKxUYqK0ZInUrZtUsqS54W50tN2VAgDgeQhOAJDDBAZKDzxgroOKjjbXRRUvLp04If33v1KlSlKzZuamu3FxdlcLAIBnIDgBQA5WoYLpxHfkiPTVV1LnzpKXl/Tdd1LfvqbhxIABZpkfbc0BAMg6ghMAeABfX6lrV+nrr6XDh6Vx46SyZaXYWGnqVKluXalWLWnyZOnvv+2uFgCAnIfgBAAepkQJacQIad8+afVqqWdPyc9P2rFDeuIJMwv1wAPSmjW0NQcAILMITgDgoby8pJYtpU8/NQ0lJk0yN9NNSDD7WrUyS/1efdW8DgAAro/gBAC5QKFCZrZp+3Zp82bp3/+WgoKk3383s1MREdKdd0qLFkmXL9tdLQAA7ofgBAC5iMNhrneaOtXcXHfGDKlJE+nKFXN9VNeuJkQNHy799pvd1QIA4D4ITgCQS+XNK/XpYzrw7dkjDR0qFS1qAtX48VL58lKLFtLHH5smEwAA5GYEJwCAKlUy94D64w9p4UKpY0dzjdTatdLDD5v7RN13n/TNN5LTaXe1AAC4HsEJAJAsTx6pWzdpyRLp4EHplVekypWl+Hhp3jypSxfTte/JJ6VNm7g3FAAg9yA4AQDSFB5urnXavVvaskUaPFgqVkz66y/pnXekBg3MTNWYMdKBA3ZXCwDArUVwAgCky+GQ6tSRJkyQ/vxTWrrU3BsqIED69Vdp5Ehzs92mTaX33+cGuwAAz0RwAgBkmo+P1KGDuQ/U8ePSzJlSmzYmXH3/vfTYY1JIiFnu9+WX5p5RAAB4AoITACBLgoKkBx+UVq6UjhwxzSWqV5cuXTKhqVs3KTTUhKkffuB6KABAzkZwAgDctBIlTDvzHTvM9uyzUliYWbb3/vvmXlG33WaW9f36q93VAgBw4whOAIBsVb269Prr0uHDZjaqTx8pXz7TQGLMGKliRdNYYvJk02gCAICcgOAEALglvL3N9U8zZkjHjklz5pj7Q3l7m1bmTzxhZqW6dJHmz5cuXrS7YgAAro/gBAC45fLmle6/39wf6s8/pYkTTae+y5fNTXXvvdc0lXj4YSkqSkpMtLtiAABSIjgBAFyqeHHpqafMvaF++UUaMUKKiJBiY6WPP5ZatpRKlzb3kPrlF7urBQDAIDgBAGxTubI0bpy5/mntWumRR6T8+U2XvvHjpapVpdq1zT2kjh2zu1oAQG5GcAIA2M7LS2rWTPrwQxOQFiyQ7rzT3Ddq2zZpyBDTua9zZ2+tXVtS58/bXTEAILchOAEA3Iq/v3TPPdL//Z8UEyO9+67UsKG57mnFCi9NmFBHJUr46N57pa++4ia7AADXIDgBANxWkSLSwIHShg3m/k8vvnhFoaHndfGiQ/PnS3ffba6ZeughacUK02wCAIBbgeAEAMgRypeXRo5M1HvvfasNGy7rmWfM8r2zZ03L8/btTXvzQYOk77+nMx8AIHsRnAAAOYrDIdWpY+mNN8xNdteulQYMMLNTf/0lvfee1LSp6cz37LPS1q2SZdldNQAgpyM4AQByrKSmEu+9Jx09Ki1dKvXpIwUHm858b7xh7hdVqZI0apS0d6/dFQMAciqCEwDAI/j6Sh06mGV7x49LCxdK//qXaTbx66/Sf/5j2p/XrCm99pp06JDdFQMAchKCEwDA4/j7S926SfPnSydOSJ98It1xh2lvvmOH9PzzZilfo0bSO++YoAUAQHoITgAAjxYUJD3wgPTNN+YeUe+/L7Vsaa6V2rBBevJJ01SiTRtp2jTp77/trhgA4I4ITgCAXKNwYal/f2n1aumPP6QJE6QGDUwHvm+/lR55xLQ3v/NO6bPPpAsX7K4YAOAuCE4AgFwpLEwaPFjauFH6/Xdp3Djp9tslp1P6+mupZ0+pWDHp/vvNzXi50S4A5G4EJwBArle2rDRihLRzp/Tzz9KLL0q33SbFxUlz50p33SWFhEgPPyytXMmNdgEgNyI4AQDwD1WrSmPGSPv2SZs2SU8/bWanzpyRPv5YatfO3Hj3iSekH37gRrsAkFsQnAAASIPDIdWrJ731lrknVFSU9O9/m+ukTpyQJk+WmjSRypSRnnvOLPkjRAGA5yI4AQCQAS8vqXlzaepUKSZGWrJE6t3bdOw7fFh6/XUpMtLMTD36qLlG6uJFu6sGAGQnghMAADfA11fq2FGaNcvc/+nzz6V775WCg83zjz4yXfkKFzbXRk2fbmaoAAA5m63BacqUKapevbqCg4MVHBysyMhILV269LrHR0VFyeFwpNr27t3rwqoBADACAqTu3U0Dib/+klaskB5/XIqIMDNO//d/Ur9+prFEkyZmZio62u6qAQBZYWtwKlmypMaPH68tW7Zoy5YtatWqlbp27ardu3ene150dLRiYmKSt/Lly7uoYgAA0pYnj9S2rfTOO9LBg9K2bdLLL0u1a0uWZRpJPPecVKmSVLGiNGyY9P330pUrdlcOAMgMW4NTly5d1KlTJ1WoUEEVKlTQuHHjlC9fPm3cuDHd84oVK6aQkJDkzdvb20UVAwCQMYdDqllTGjlS+ukncx3Uu+9K7dubpX6//ir9979S06ZmNuqhh6Qvv+SGuwDgznzsLiDJlStXtGDBAl24cEGRkZHpHlurVi3Fx8erSpUqevHFF9WyZcvrHpuQkKCEf9y1MDY2VpLkdDrldDqzp3ikKWl8GWfXYcxdjzF3rZw63iEhpmnEo49KsbHSihUOff21l5YudejkSYdmzJBmzJD8/Cy1bm2pS5dEdepkKTTU7spz7pjnZIy5azHerudOY34jNTgsy7JuYS0Z2rVrlyIjIxUfH698+fJpzpw56tSpU5rHRkdHa926dapTp44SEhL0ySefaOrUqYqKilKzZs3SPGf06NF6+eWXU+2fM2eOAgMDs/WzAABwIy5fdmjPnsLatClEmzaF6PjxvCler1DhtOrVO6b69Y8pIuKcHA6bCgUADxUXF6eePXvq7NmzCg4OTvdY24PTpUuXdPjwYZ05c0YLFy7URx99pLVr16pKlSqZOr9Lly5yOBxatGhRmq+nNeMUHh6ukydPZjg4uDlOp1MrV65U27Zt5evra3c5uQJj7nqMuWt58nhblrR7t/T111765huHNm9OuZq+bFlLnTsnqksXS40bW/Jx0ZoRTx5zd8WYuxbj7XruNOaxsbEqUqRIpoKT7Uv18uTJo3LlykmS6tatq82bN+vtt9/W+++/n6nzGzZsqNmzZ1/3dT8/P/n5+aXa7+vra/sfVG7BWLseY+56jLlreep416pltpEjpaNHpW++kRYtklatkvbvd2jSJG9NmiQVLCjdcYdpe96+vWmFfqt56pi7M8bctRhv13OHMb+Rn+9293GyLCvFDFFGtm3bplB3WAQOAEA2CguT+vc34enkSemLL6Q+fcz9of7+W5o9W+rRQypaVOrQQXrvPemPP+yuGgA8l60zTiNGjFDHjh0VHh6uc+fOae7cuYqKitKyZcskScOHD9eff/6pWbNmSZImTpyo0qVLq2rVqrp06ZJmz56thQsXauHChXZ+DAAAbql8+aS77zbblSvS+vVmJur//k/at09avtxsgwaZ9uddu5rZqBo1xHVRAJBNbA1Ox48fV+/evRUTE6P8+fOrevXqWrZsmdq2bStJiomJ0eHDh5OPv3TpkoYOHao///xTAQEBqlq1qhYvXnzdZhIAAHgab2/Txrxp06s31P2//zNBasMGaetWs40aJZUoYWajOnQw95jKn9/u6gEg57I1OE2bNi3d12fMmJHi+bBhwzRs2LBbWBEAADmHw2FuqFupkrm57vHj0uLFJkStWCH9+ac0bZrZvL2lRo2kjh3NxmwUANwYt7vGCQAAZE3x4lK/ftJXX0mnT5vle4MHSxUrmiV+330njRhhGlCEhZkb786fb66ZAgCkj+AEAIAH8veX2rWTJkyQ9u6V9u+X3n1X6tJFCgyUjh0zN929916pSBGpSRNp7Fjpp5+kxES7qwcA90NwAgAgFyhTRho40CzjO31aWrlSGjJEqlzZBKUffpBeekmqW9fMRvXpI82da44FALjBfZwAAIBr+flJbdqY7c03pUOHpGXLpKVLpW+/NddKzZplNi8vH5Uv31Rbt3qpc2epTh3Ji392BZAL8VcfAAC5XKlS0r//ba6NOnXKhKdnn5WqVZMSEx2Kji6k//zHW/Xrm+uoHnhA+vRTc38pAMgtCE4AACBZnjxSq1am1fmuXdLvvzs1cOB23XVXooKCTFj69FMTnooVkxo0kEaPljZuNA0oAMBTEZwAAMB1hYdL7dod0vz5V3TqlBQVZVqfV68uWZa0aZP08stSZKSZjerZU/rkE+nECbsrB4DsRXACAACZ4usrNW8ujR8v7dhx9T5R99xjbq576pT02WfSgw+aEFWvnmk4sX49s1EAcj6CEwAAyJKwMHPfqAULzBK+f94nSpK2bDEtzhs3looWle67z7RAj4mxtWwAyBKCEwAAuGk+PuZeUOPGSVu3SkePStOnSz16SAUKmJvszptnbrobFibVqGGW/K1ZI126ZHf1AJAxghMAAMh2oaFS374mLP31l7lP1IsvmuV7Doe0c6dpQNGqlVSokHTnndJ770m//2535QCQNu7jBAAAbikfH6lRI7ONGWOW9a1cae4dtXy5uW/U11+bTZLKlZPat5c6dJBatJDy5bO1fACQRHACAAAuVqSIdP/9ZktMNLNPSSHq+++l334z27vvmvboTZqYENWhg7m3lMNh9ycAkBuxVA8AANjGy0uqWVN6/nlzvdPp0+ZGvAMGSGXKmOufVq+Whg0zLdBLljQNKebPN8cCgKsw4wQAANxGUJDUtavZLMvMPC1bZrY1a642nZg+3YSu+vXNTFT79ub6KW9vuz8BAE9FcAIAAG7J4ZDKlzfbE09I8fFmKV/Ssr6ff5Y2bjTb6NFSwYJS27ZXg1RYmN2fAIAnYakeAADIEfz9pTZtpDfekHbtko4cMTfg/de/rrY8nz/fLOUrUcIs7Rs2zCz1S0iwu3oAOR3BCQAA5Ej/vN7pr7+k9eulkSPN8j2Hw4Sr//5Xat3atDzv0kWaPNks/wOAG8VSPQAAkOP5+EiRkWZ7+WXp1CnT8nz5crO079gx6ZtvzCZJt92WsuV5UJCt5QPIAQhOAADA4xQuLN13n9ksy7Q8TwpR339vbrT73ntm8/aW6tSRWrY0IapxY4IUgNQITgAAwKM5HFKNGmYbNkw6f9506EtqMvH779KmTWZ77TUTpOrVMyEqKUhxE14ABCcAAJCr5Mtnrnfq0sU8P3JEioq6uu3ff7Vb3/jxZhlgvXpXZ6QaNZLy5rWvfgD2IDgBAIBcLTxc6t3bbJJ06JC0dq2ZlYqKkg4elDZsMNsrr0i+vqYBRdKMVKNGUmCgffUDcA2CEwAAwD+UKiU9+KDZJBOckmaj1qyRDh+WfvjBbOPGmSDVoMHVGanISCkgwL76AdwaBCcAAIB0lC4t9e1rNssyQSppNmrNGumPP0zDie+/l8aMkfLkkRo2NCGqZUvz2N/fzk8AIDsQnAAAADLJ4ZDKlDFbv34mSO3ffzVErVkjHT0qrVtntv/8R/LzM+EpaUaqYUOzD0DOQnACAADIIofD3BPqttukhx82Qer331POSMXEmGum1q415/j7m+V8STNS9esTpICcgOAEAACQTRwOqVw5sz36qAlS+/ZdDVFRUeZmvEmzU6NGmSDVqNHVGalatWz+EADSRHACAAC4RRwOqUIFs/Xvb4JUdHTKZhMnTkirV5tNkgICfFShQqR27vRSmzZS3bqmAQUAexGcAAAAXMThkCpVMttjj5kgtXfv1dmoqCjpr78c2rGjmHbskEaONPeMatLEzEi1bCnVrm3uLQXAtfifHQAAgE0cDqlyZbMNHGiC1I4dTr333h799Vc1ffedl06dkpYvN5skBQVJzZpdvUaqZk3J29vOTwHkDgQnAAAAN+FwSFWrSnfccUCdOlWWt7eXfv756jVRa9dKZ85IixebTZIKFEgZpKpXl7y8bPwQgIciOAEAALgpLy8ThKpXl556SrpyRdqx4+r1UevWmSC1aJHZJKlQIal586vNJqpWJUgB2YHgBAAAkEN4e5trnGrXloYMkS5flrZtu3qN1HffSadPS19+aTZJKlLk6mxUy5bm+iqHw85PAeRMBCcAAIAcysdHqlfPbMOGSU6n9NNPV5f2/fCDdPKk9PnnZpOk4sVTBqny5QlSQGYQnAAAADyEr6/UsKHZhg+XLl2SNm++GqTWr5eOH5fmzTObJIWFpQxSZcsSpIC0EJwAAAA8VJ48UuPGZnvxRSk+Xvrxx6vXSG3YIB09Ks2ZYzZJCg+/en1Uy5ZS6dI2fgDAjRCcAAAAcgl/f9M4onlzadQo6eJFE56SrpH68UfpyBFp1iyzSSY4tWxpOvc1asTSPuReBCcAAIBcKiBAatXKbJJ04YJZzpe0tG/zZungQWn6dLNJptlEZKQJUY0aSXXrSoGBtn0EwGUITgAAAJAk5c0rtW1rNkk6d840mEi6PmrzZtNs4uuvzSaZBhW1al0NUo0aSSVL2vcZgFuF4AQAAIA0BQVJHTqYTZISEqTt202IWr/ehKqYGBOoNm+W3n7bHBcRkTJIVa9uGlcAORnBCQAAAJni5yc1aGC2p5+WLEs6fNgEqKQwtWOH2Xf4sDR3rjkvMFCqX/9qkGrYUCpc2N7PAtwoghMAAACyxOGQSpUyW8+eZt/589KmTVeD1IYN0pkzpvlEVNTVcytVSjkrVbGi5OVlw4cAMongBAAAgGyTL1/KhhOJidLevVeD1Pr1UnS02bd3r/Txx+a4ggVTNp2oX99ccwW4C4ITAAAAbhkvL6lKFbM98ojZd/KktHHj1SC1aZP099/SkiVmkyRvb6lGjZSzUhERtEKHfQhOAAAAcKkiRaTOnc0mSU6nuTbqn7NSR45IW7eabfJkc1xYWMogVauWuckv4AoEJwAAANjK19fcD6puXenJJ82+I0fM9VFJQWrbNunoUenzz80mmRv61q2bMkwVLWrf54BnIzgBAADA7YSHm61HD/M8Lk7asiXlrNSpU9L335stSfnyKYNUlSo0nUD2IDgBAADA7QUGSs2amU0yrdD37UsZpHbvNvv27ZNmzjTH5c9v2p8nBakGDcz9qYAbRXACAABAjuNwSBUqmK1vX7Pv77+lH3+8GqQ2bpTOnpWWLzebZGafbr/9apCqV8+EMCAjBCcAAAB4hIIFpQ4dzCZJly9Lu3alnJU6eNA0otixQ5oyRZJ8VaBAezVv7q2mTU2Yql3b3OwX+CeCEwAAADySj4/pvFerljRokNl39GjKphM//WTpzBl//d//Sf/3f+aYPHlSNp2IjJRCQuz7HHAPBCcAAADkGmFhUvfuZpOkc+cu6913N0pqpB9/9NYPP0h//XU1WCUpWzZl04lq1cy9ppB7EJwAAACQa/n7S5Urn1anTony9fWWZUn796dc3rdrl9m3f780e7Y5L1++lE0nGjY0jSjguQhOAAAAwP84HNJtt5mtd2+z7+zZ1E0nzp2TVq0yW9J5VauaAJW0PLB6dSlvXvs+C7IXwQkAAABIR/78Urt2ZpOkK1dM6/N/zkr9/rv0889mS+LlZbr+1a59NUzVqiUVKmTP58DNITgBAAAAN8Db28wmVa8uPfaY2Xf8eFKzCWnrVmnbNunYMWnvXrPNmXP1/IiI1GGqRAkzawX3RXACAAAAblLx4tLdd5stybFjJkBt23Y1TO3fLx0+bLavvrp6bJEiV0NUUqgqV87MWsE9EJwAAACAWyAkROrY0WxJzp6Vtm9PGab27JFOnpRWrjRbknz5pBo1UoapKlVMu3S4HsEJAAAAcJH8+aXmzc2W5OJFc23UP8PUzp3S+fPSDz+YLUmePKYJxT9np6pXNyELtxbBCQAAALBRQIBUr57Zkly+LEVHpwxT27aZGaukx0kcDtOE4tqlfoULu/6zeDKCEwAAAOBmfHzMzFLVqtIDD5h9liUdPJg6TMXEmJAVHS3NnXv1PcLDrwappC0sjCYUWWXr5WZTpkxR9erVFRwcrODgYEVGRmrp0qXpnrN27VrVqVNH/v7+Klu2rKZOneqiagEAAAD7OBxSmTJSt27S2LHS4sXS0aOmCcXSpdK4cdI995h7UEnSkSPSokXS6NHSnXdKJUteve7qhRekhQulAwdMIEPGbJ1xKlmypMaPH69y5cpJkmbOnKmuXbtq27Ztqlq1aqrjDxw4oE6dOunRRx/V7Nmz9cMPP2jgwIEqWrSounfv7uryAQAAANsVLy516GC2JGfPSjt2XJ2Z2rpV+uUX6cQJadkysyUpUCD1zFT58qbtOq6yNTh16dIlxfNx48ZpypQp2rhxY5rBaerUqYqIiNDEiRMlSZUrV9aWLVv0xhtvEJwAAACA/8mfX2rWzGxJ4uKkXbtMiEoKVLt2SWfOSGvWmC1J3rxSzZopw1TlypKvr6s/iftwm2ucrly5ogULFujChQuKjIxM85gNGzaoXdItm/+nffv2mjZtmpxOp3zT+JNMSEhQQkJC8vPY2FhJktPplNPpzMZPgGsljS/j7DqMuesx5q7FeLseY+56jLlr5abx9vW9GoKSXLpkZqK2b3do2zaz7djh0IULjlQd/fz8LN1+u6VatZI2qWpVS/7+N1aHO435jdTgsCx7VzXu2rVLkZGRio+PV758+TRnzhx16tQpzWMrVKigvn37asSIEcn71q9fr8aNG+vo0aMKDQ1Ndc7o0aP18ssvp9o/Z84cBQYGZt8HAQAAADzAlSvS0aNB+v33/Nq/P79+/72ADhzIr7i41JMU3t6JCg8/p7Jlz6ps2TO67bazKl36rAICrthQ+Y2Li4tTz549dfbsWQUHB6d7rO0zThUrVtT27dt15swZLVy4UH369NHatWtVpUqVNI93XNMGJCn3Xbs/yfDhwzVkyJDk57GxsQoPD1e7du0yHBzcHKfTqZUrV6pt27ZpzgYi+zHmrseYuxbj7XqMuesx5q7FeGdOYqK0f78zeVYqaYbq1CkvHTyYXwcP5tfq1RGSJIfD+l979KuzUzVrWipQwLyXO4150mq0zLA9OOXJkye5OUTdunW1efNmvf3223r//fdTHRsSEqJjx46l2HfixAn5+Pio8HUa1fv5+cnPzy/Vfl9fX9v/oHILxtr1GHPXY8xdi/F2Pcbc9Rhz12K8M1a5stl69jTPLct07ktqPpG0HT3q+F97dEeK9uhly5plgjVqeOny5aJq3NhXRYrYO+Y38mdue3C6lmVZKa5J+qfIyEh9/fXXKfatWLFCdevW5YsOAAAAuJDDIUVEmK1r16v7jx1LHaYOHpT27zfb5597S2qkunUvq3Nnu6q/cbYGpxEjRqhjx44KDw/XuXPnNHfuXEVFRWnZ//ojDh8+XH/++admzZolSXrsscc0efJkDRkyRI8++qg2bNigadOm6bPPPrPzYwAAAAD4n6R7RXXseHXf6dNXw9SWLYn6/vs41ayZelWYO7M1OB0/fly9e/dWTEyM8ufPr+rVq2vZsmVq27atJCkmJkaHDx9OPr5MmTJasmSJnn76ab377rsKCwvTpEmTaEUOAAAAuLFChaTWrc3mdF7RkiXfqnjxtBvCuStbg9O0adPSfX3GjBmp9jVv3lxbt269RRUBAAAAQGpedhcAAAAAAO6O4AQAAAAAGSA4AQAAAEAGCE4AAAAAkAGCEwAAAABkgOAEAAAAABkgOAEAAABABghOAAAAAJABghMAAAAAZIDgBAAAAAAZIDgBAAAAQAYITgAAAACQAYITAAAAAGSA4AQAAAAAGSA4AQAAAEAGCE4AAAAAkAGCEwAAAABkwMfuAlzNsixJUmxsrM2VeD6n06m4uDjFxsbK19fX7nJyBcbc9Rhz12K8XY8xdz3G3LUYb9dzpzFPygRJGSE9uS44nTt3TpIUHh5ucyUAAAAA3MG5c+eUP3/+dI9xWJmJVx4kMTFRR48eVVBQkBwOh93leLTY2FiFh4fryJEjCg4OtrucXIExdz3G3LUYb9djzF2PMXctxtv13GnMLcvSuXPnFBYWJi+v9K9iynUzTl5eXipZsqTdZeQqwcHBtv+PIrdhzF2PMXctxtv1GHPXY8xdi/F2PXcZ84xmmpLQHAIAAAAAMkBwAgAAAIAMEJxwy/j5+WnUqFHy8/Ozu5RcgzF3PcbctRhv12PMXY8xdy3G2/Vy6pjnuuYQAAAAAHCjmHECAAAAgAwQnAAAAAAgAwQnAAAAAMgAwQkAAAAAMkBwQpa8+uqrqlevnoKCglSsWDHdddddio6OTvecqKgoORyOVNvevXtdVHXONnr06FRjFxISku45a9euVZ06deTv76+yZctq6tSpLqrWM5QuXTrN7+ygQYPSPJ7v+I1Zt26dunTporCwMDkcDn311VcpXrcsS6NHj1ZYWJgCAgLUokUL7d69O8P3XbhwoapUqSI/Pz9VqVJFX3755S36BDlPemPudDr13HPP6fbbb1fevHkVFhamBx98UEePHk33PWfMmJHm9z4+Pv4Wf5qcIaPved++fVONXcOGDTN8X77n15fRmKf1fXU4HPrvf/973ffke359mfmd0FP+Pic4IUvWrl2rQYMGaePGjVq5cqUuX76sdu3a6cKFCxmeGx0drZiYmOStfPnyLqjYM1StWjXF2O3ateu6xx44cECdOnVS06ZNtW3bNo0YMUJPPvmkFi5c6MKKc7bNmzenGO+VK1dKkv71r3+lex7f8cy5cOGCatSoocmTJ6f5+uuvv6633npLkydP1ubNmxUSEqK2bdvq3Llz133PDRs26N5771Xv3r21Y8cO9e7dWz169NCPP/54qz5GjpLemMfFxWnr1q166aWXtHXrVn3xxRf69ddfdeedd2b4vsHBwSm+8zExMfL3978VHyHHyeh7LkkdOnRIMXZLlixJ9z35nqcvozG/9rv68ccfy+FwqHv37um+L9/ztGXmd0KP+fvcArLBiRMnLEnW2rVrr3vMmjVrLEnW33//7brCPMioUaOsGjVqZPr4YcOGWZUqVUqx79///rfVsGHDbK4s93jqqaes2267zUpMTEzzdb7jWSfJ+vLLL5OfJyYmWiEhIdb48eOT98XHx1v58+e3pk6det336dGjh9WhQ4cU+9q3b2/dd9992V5zTnftmKdl06ZNliTr0KFD1z1m+vTpVv78+bO3OA+V1pj36dPH6tq16w29D9/zzMvM97xr165Wq1at0j2G73nmXfs7oSf9fc6ME7LF2bNnJUmFChXK8NhatWopNDRUrVu31po1a251aR5l3759CgsLU5kyZXTfffdp//791z12w4YNateuXYp97du315YtW+R0Om91qR7n0qVLmj17tvr16yeHw5HusXzHb96BAwd07NixFN9hPz8/NW/eXOvXr7/uedf73qd3Dq7v7NmzcjgcKlCgQLrHnT9/XqVKlVLJkiXVuXNnbdu2zTUFeoioqCgVK1ZMFSpU0KOPPqoTJ06kezzf8+xz/PhxLV68WA8//HCGx/I9z5xrfyf0pL/PCU64aZZlaciQIWrSpImqVat23eNCQ0P1wQcfaOHChfriiy9UsWJFtW7dWuvWrXNhtTlXgwYNNGvWLC1fvlwffvihjh07pkaNGunUqVNpHn/s2DEVL148xb7ixYvr8uXLOnnypCtK9ihfffWVzpw5o759+173GL7j2efYsWOSlOZ3OOm16513o+cgbfHx8Xr++efVs2dPBQcHX/e4SpUqacaMGVq0aJE+++wz+fv7q3Hjxtq3b58Lq825OnbsqE8//VSrV6/Wm2++qc2bN6tVq1ZKSEi47jl8z7PPzJkzFRQUpG7duqV7HN/zzEnrd0JP+vvcx7afDI/x+OOPa+fOnfr+++/TPa5ixYqqWLFi8vPIyEgdOXJEb7zxhpo1a3ary8zxOnbsmPz49ttvV2RkpG677TbNnDlTQ4YMSfOca2dGLMtKcz8yNm3aNHXs2FFhYWHXPYbvePZL6zuc0fc3K+cgJafTqfvuu0+JiYl677330j22YcOGKZoZNG7cWLVr19Y777yjSZMm3epSc7x77703+XG1atVUt25dlSpVSosXL073l3m+59nj448/Vq9evTK8Vonveeak9zuhJ/x9zowTbsoTTzyhRYsWac2aNSpZsuQNn9+wYUP+tSaL8ubNq9tvv/264xcSEpLqX2VOnDghHx8fFS5c2BUleoxDhw5p1apVeuSRR274XL7jWZPUMTKt7/C1/wJ57Xk3eg5Scjqd6tGjhw4cOKCVK1emO9uUFi8vL9WrV4/vfRaFhoaqVKlS6Y4f3/Ps8d133yk6OjpLf7fzPU/ter8TetLf5wQnZIllWXr88cf1xRdfaPXq1SpTpkyW3mfbtm0KDQ3N5upyh4SEBO3Zs+e64xcZGZncBS7JihUrVLduXfn6+rqiRI8xffp0FStWTHfccccNn8t3PGvKlCmjkJCQFN/hS5cuae3atWrUqNF1z7ve9z69c3BVUmjat2+fVq1alaV/ZLEsS9u3b+d7n0WnTp3SkSNH0h0/vufZY9q0aapTp45q1Khxw+fyPb8qo98JPervc3t6UiCnGzBggJU/f34rKirKiomJSd7i4uKSj3n++eet3r17Jz+fMGGC9eWXX1q//vqr9fPPP1vPP/+8JclauHChHR8hx3nmmWesqKgoa//+/dbGjRutzp07W0FBQdbBgwcty0o93vv377cCAwOtp59+2vrll1+sadOmWb6+vtbnn39u10fIka5cuWJFRERYzz33XKrX+I7fnHPnzlnbtm2ztm3bZkmy3nrrLWvbtm3JHdzGjx9v5c+f3/riiy+sXbt2Wffff78VGhpqxcbGJr9H7969reeffz75+Q8//GB5e3tb48ePt/bs2WONHz/e8vHxsTZu3Ojyz+eO0htzp9Np3XnnnVbJkiWt7du3p/i7PSEhIfk9rh3z0aNHW8uWLbN+//13a9u2bdZDDz1k+fj4WD/++KMdH9HtpDfm586ds5555hlr/fr11oEDB6w1a9ZYkZGRVokSJfie34SM/m6xLMs6e/asFRgYaE2ZMiXN9+B7nnmZ+Z3QU/4+JzghSySluU2fPj35mD59+ljNmzdPfv7aa69Zt912m+Xv728VLFjQatKkibV48WLXF59D3XvvvVZoaKjl6+trhYWFWd26dbN2796d/Pq1421ZlhUVFWXVqlXLypMnj1W6dOnr/h8Erm/58uWWJCs6OjrVa3zHb05S+/Zrtz59+liWZVrYjho1ygoJCbH8/PysZs2aWbt27UrxHs2bN08+PsmCBQusihUrWr6+vlalSpUIrv+Q3pgfOHDgun+3r1mzJvk9rh3zwYMHWxEREVaePHmsokWLWu3atbPWr1/v+g/nptIb87i4OKtdu3ZW0aJFLV9fXysiIsLq06ePdfjw4RTvwff8xmT0d4tlWdb7779vBQQEWGfOnEnzPfieZ15mfif0lL/PHZb1v6vFAQAAAABp4honAAAAAMgAwQkAAAAAMkBwAgAAAIAMEJwAAAAAIAMEJwAAAADIAMEJAAAAADJAcAIAAACADBCcAAAAACADBCcAANLhcDj01Vdf2V0GAMBmBCcAgNvq27evHA5Hqq1Dhw52lwYAyGV87C4AAID0dOjQQdOnT0+xz8/Pz6ZqAAC5FTNOAAC35ufnp5CQkBRbwYIFJZlldFOmTFHHjh0VEBCgMmXKaMGCBSnO37Vrl1q1aqWAgAAVLlxY/fv31/nz51Mc8/HHH6tq1ary8/NTaGioHn/88RSvnzx5UnfffbcCAwNVvnx5LVq0KPm1v//+W7169VLRokUVEBCg8uXLpwp6AICcj+AEAMjRXnrpJXXv3l07duzQAw88oPvvv1979uyRJMXFxalDhw4qWLCgNm/erAULFmjVqlUpgtGUKVM0aNAg9e/fX7t27dKiRYtUrly5FD/j5ZdfVo8ePbRz50516tRJvXr10unTp5N//i+//KKlS5dqz549mjJliooUKeK6AQAAuITDsizL7iIAAEhL3759NXv2bPn7+6fY/9xzz+mll16Sw+HQY489pilTpiS/1rBhQ9WuXVvvvfeePvzwQz333HM6cuSI8ubNK0lasmSJunTpoqNHj6p48eIqUaKEHnroIY0dOzbNGhwOh1588UWNGTNGknThwgUFBQVpyZIl6tChg+68804VKVJEH3/88S0aBQCAO+AaJwCAW2vZsmWKYCRJhQoVSn4cGRmZ4rXIyEht375dkrRnzx7VqFEjOTRJUuPGjZWYmKjo6Gg5HA4dPXpUrVu3TreG6tWrJz/OmzevgoKCdOLECUnSgAED1L17d23dulXt2rXTXXfdpUaNGmXpswIA3BfBCQDg1vLmzZtq6VxGHA6HJMmyrOTHaR0TEBCQqffz9fVNdW5iYqIkqWPHjjp06JAWL16sVatWqXXr1ho0aJDeeOONG6oZAODeuMYJAJCjbdy4MdXzSpUqSZKqVKmi7du368KFC8mv//DDD/Ly8lKFChUUFBSk0qVL69tvv72pGooWLZq8rHDixIn64IMPbur9AADuhxknAIBbS0hI0LFjx1Ls8/HxSW7AsGDBAtWtW1dNmjTRp59+qk2bNmnatGmSpF69emnUqFHq06ePRo8erb/++ktPPPGEevfureLFi0uSRo8erccee0zFihVTx44dde7cOf3www964oknMlXfyJEjVadOHVWtWlUJCQn65ptvVLly5WwcAQCAOyA4AQDc2rJlyxQaGppiX8WKFbV3715JpuPd3LlzNXDgQIWEhOjTTz9VlSpVJEmBgYFavny5nnrqKdWrV0+BgYHq3r273nrrreT36tOnj+Lj4zVhwgQNHTpURYoU0T333JPp+vLkyaPhw4fr4MGDCggIUNOmTTV37txs+OQAAHdCVz0AQI7lcDj05Zdf6q677rK7FACAh+MaJwAAAADIAMEJAAAAADLANU4AgByL1eYAAFdhxgkAAAAAMkBwAgAAAIAMEJwAAAAAIAMEJwAAAADIAMEJAAAAADJAcAIAAACADBCcAAAAACADBCcAAAAAyMD/A6CY4QdJcG2eAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Loss values from your training output (replace with the actual loss values you have)\n",
    "train_losses = [\n",
    "    5.847795763905369, 4.914523594290463, 4.5751000347600055, 4.352262926599873,\n",
    "    4.184005458959892, 4.042845732934439, 3.918032662388104, 3.8128471590867683,\n",
    "    3.7139017724457073, 3.621993897566155, 3.534025015350598, 3.4561936408325806,\n",
    "    3.383977654051425, 3.312921400176945, 3.2490196408353635, 3.184731990588245,\n",
    "    3.1271846369531615, 3.0695784538717414, 3.020777788633731, 2.9712336649378734\n",
    "]\n",
    "\n",
    "# Plotting the training loss curve\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(range(1, len(train_losses) + 1), train_losses, label='Training Loss', color='blue')\n",
    "\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Loss Curve')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=\"seagreen\"> **Attention Visualization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\"\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.d_k = d_model // num_heads\n",
    "        \n",
    "        self.q_linear = nn.Linear(d_model, d_model)\n",
    "        self.k_linear = nn.Linear(d_model, d_model)\n",
    "        self.v_linear = nn.Linear(d_model, d_model)\n",
    "        self.out = nn.Linear(d_model, d_model)\n",
    "        \n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "    \n",
    "    def attention(self, query, key, value, mask=None):\n",
    "        # Scaled dot-product attention\n",
    "        scores = torch.matmul(query, key.transpose(-2, -1)) / torch.sqrt(torch.tensor(self.d_k, dtype=torch.float32))\n",
    "        \n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask == 0, float('-inf'))\n",
    "        \n",
    "        attn_weights = F.softmax(scores, dim=-1)\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "        output = torch.matmul(attn_weights, value)\n",
    "        return output, attn_weights\n",
    "\n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        batch_size = query.size(0)\n",
    "        \n",
    "        # Linear transformations\n",
    "        query = self.q_linear(query).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        key = self.k_linear(key).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        value = self.v_linear(value).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        \n",
    "        # Scaled dot-product attention\n",
    "        attn_output, attn_weights = self.attention(query, key, value, mask)\n",
    "        \n",
    "        # Concatenate heads and pass through final linear layer\n",
    "        attn_output = attn_output.transpose(1, 2).contiguous().view(batch_size, -1, self.d_model)\n",
    "        attn_output = self.out(attn_output)\n",
    "        return attn_output, attn_weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer1(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):\n",
    "        super(EncoderLayer1, self).__init__()\n",
    "        self.self_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.ff = FeedForward(d_model, d_ff, dropout)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        attn_output, attn_weights = self.self_attn(x, x, x, mask)\n",
    "        x = self.norm1(x + self.dropout(attn_output))\n",
    "        ff_output = self.ff(x)\n",
    "        x = self.norm2(x + self.dropout(ff_output))\n",
    "        return x, attn_weights  # Return attention weights\n",
    "\n",
    "class DecoderLayer1(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):\n",
    "        super(DecoderLayer1, self).__init__()\n",
    "        self.self_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.enc_dec_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.ff = FeedForward(d_model, d_ff, dropout)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.norm3 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, enc_output, src_mask=None, tgt_mask=None):\n",
    "        self_attn_output, self_attn_weights = self.self_attn(x, x, x, tgt_mask)\n",
    "        x = self.norm1(x + self.dropout(self_attn_output))\n",
    "\n",
    "        enc_dec_attn_output, enc_dec_attn_weights = self.enc_dec_attn(x, enc_output, enc_output, src_mask)\n",
    "        x = self.norm2(x + self.dropout(enc_dec_attn_output))\n",
    "\n",
    "        ff_output = self.ff(x)\n",
    "        x = self.norm3(x + self.dropout(ff_output))\n",
    "        return x, self_attn_weights, enc_dec_attn_weights  # Return both self and enc-dec attention weights\n",
    "\n",
    "class Encoder1(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, num_heads, num_layers, d_ff, max_len, dropout=0.1):\n",
    "        super(Encoder1, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.pos_encoding = PositionalEncoding(d_model, max_len)\n",
    "        self.layers = nn.ModuleList([EncoderLayer1(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)])\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        x = self.embedding(x) * math.sqrt(self.embedding.embedding_dim)\n",
    "        x = self.pos_encoding(x)\n",
    "        all_attn_weights = []\n",
    "        for layer in self.layers:\n",
    "            x, attn_weights = layer(x, mask)\n",
    "            all_attn_weights.append(attn_weights)\n",
    "        return x, all_attn_weights  # Return all attention weights\n",
    "\n",
    "class Decoder1(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, num_heads, num_layers, d_ff, max_len, dropout=0.1):\n",
    "        super(Decoder1, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.pos_encoding = PositionalEncoding(d_model, max_len)\n",
    "        self.layers = nn.ModuleList([DecoderLayer1(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)])\n",
    "\n",
    "    def forward(self, x, enc_output, src_mask=None, tgt_mask=None):\n",
    "        x = self.embedding(x) * math.sqrt(self.embedding.embedding_dim)\n",
    "        x = self.pos_encoding(x)\n",
    "        all_self_attn_weights = []\n",
    "        all_enc_dec_attn_weights = []\n",
    "        for layer in self.layers:\n",
    "            x, self_attn_weights, enc_dec_attn_weights = layer(x, enc_output, src_mask, tgt_mask)\n",
    "            all_self_attn_weights.append(self_attn_weights)\n",
    "            all_enc_dec_attn_weights.append(enc_dec_attn_weights)\n",
    "        return x, all_self_attn_weights, all_enc_dec_attn_weights  # Return all attention weights\n",
    "    \n",
    "class Transformer1(nn.Module):\n",
    "    def __init__(self, src_vocab_size, tgt_vocab_size, src_pad_idx, tgt_pad_idx, d_model, num_heads, num_layers, d_ff, max_len, dropout=0.1):\n",
    "        super(Transformer1, self).__init__()\n",
    "        self.src_pad_idx = src_pad_idx\n",
    "        self.tgt_pad_idx = tgt_pad_idx\n",
    "        self.encoder = Encoder1(src_vocab_size, d_model, num_heads, num_layers, d_ff, max_len, dropout)\n",
    "        self.decoder = Decoder1(tgt_vocab_size, d_model, num_heads, num_layers, d_ff, max_len, dropout)\n",
    "        self.fc_out = nn.Linear(d_model, tgt_vocab_size)\n",
    "\n",
    "    def make_src_mask(self, src):\n",
    "        return (src != self.src_pad_idx).unsqueeze(1).unsqueeze(2)\n",
    "\n",
    "    def make_tgt_mask(self, tgt):\n",
    "        tgt_pad_mask = (tgt != self.tgt_pad_idx).unsqueeze(1).unsqueeze(2)\n",
    "        tgt_len = tgt.size(1)\n",
    "        tgt_subseq_mask = torch.tril(torch.ones((tgt_len, tgt_len), device=tgt.device)).bool()\n",
    "        return tgt_pad_mask & tgt_subseq_mask\n",
    "\n",
    "    def forward(self, src, tgt):\n",
    "        src_mask = self.make_src_mask(src)\n",
    "        tgt_mask = self.make_tgt_mask(tgt)\n",
    "        \n",
    "        # Get encoder output and attention weights\n",
    "        enc_output, enc_attn_weights = self.encoder(src, src_mask)\n",
    "        \n",
    "        # Get decoder output and attention weights\n",
    "        dec_output, dec_self_attn_weights, dec_enc_dec_attn_weights = self.decoder(tgt, enc_output, src_mask, tgt_mask)\n",
    "        \n",
    "        output = self.fc_out(dec_output)\n",
    "        \n",
    "        # Return both the transformer output and the attention weights\n",
    "        return output, (enc_attn_weights, dec_self_attn_weights, dec_enc_dec_attn_weights)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self, src_vocab_size, tgt_vocab_size, src_pad_idx, tgt_pad_idx, d_model, num_heads, num_layers, d_ff, max_len, dropout=0.1):\n",
    "        super(TransformerModel, self).__init__()\n",
    "        self.transformer = Transformer1(src_vocab_size, tgt_vocab_size, src_pad_idx, tgt_pad_idx, d_model, num_heads, num_layers, d_ff, max_len, dropout)\n",
    "\n",
    "    def forward(self, src, tgt):\n",
    "        # Forward pass through the transformer model\n",
    "        transformer_output, attention_weights = self.transformer(src, tgt)\n",
    "        # Return both the transformer output and attention weights\n",
    "        return transformer_output, attention_weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Must pass 2-d input. shape=(2, 50, 50)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 76\u001b[0m\n\u001b[0;32m     73\u001b[0m tgt_sentence \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mیہ ایک مثال جملہ ہے۔\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     75\u001b[0m \u001b[38;5;66;03m# Assuming you have a trained transformer model loaded as `model1`\u001b[39;00m\n\u001b[1;32m---> 76\u001b[0m \u001b[43mvisualize_attention\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msrc_sentence\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtgt_sentence\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[18], line 69\u001b[0m, in \u001b[0;36mvisualize_attention\u001b[1;34m(model1, src_sentence, tgt_sentence, tokenizer, device)\u001b[0m\n\u001b[0;32m     66\u001b[0m layer_idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m  \u001b[38;5;66;03m# Choose layer index (first layer)\u001b[39;00m\n\u001b[0;32m     67\u001b[0m head_idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m   \u001b[38;5;66;03m# Choose attention head index (first head)\u001b[39;00m\n\u001b[1;32m---> 69\u001b[0m \u001b[43mplot_attention_weights\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc_sentence\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtgt_sentence\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdec_enc_dec_attn_weights\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlayer_idx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43mhead_idx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[18], line 45\u001b[0m, in \u001b[0;36mplot_attention_weights\u001b[1;34m(src_sentence, tgt_sentence, attention_weights, tokenizer)\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[38;5;66;03m# Plot the attention weights\u001b[39;00m\n\u001b[0;32m     44\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m8\u001b[39m))\n\u001b[1;32m---> 45\u001b[0m \u001b[43msns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheatmap\u001b[49m\u001b[43m(\u001b[49m\u001b[43mattention_weights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mxticklabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msrc_tokens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43myticklabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtgt_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     46\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcmap\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mBlues\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcbar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mannot\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     47\u001b[0m plt\u001b[38;5;241m.\u001b[39mxlabel(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSource (English) words\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     48\u001b[0m plt\u001b[38;5;241m.\u001b[39mylabel(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTarget (Urdu) words\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\javer\\Anaconda3\\envs\\pymc5_env\\Lib\\site-packages\\seaborn\\matrix.py:446\u001b[0m, in \u001b[0;36mheatmap\u001b[1;34m(data, vmin, vmax, cmap, center, robust, annot, fmt, annot_kws, linewidths, linecolor, cbar, cbar_kws, cbar_ax, square, xticklabels, yticklabels, mask, ax, **kwargs)\u001b[0m\n\u001b[0;32m    365\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Plot rectangular data as a color-encoded matrix.\u001b[39;00m\n\u001b[0;32m    366\u001b[0m \n\u001b[0;32m    367\u001b[0m \u001b[38;5;124;03mThis is an Axes-level function and will draw the heatmap into the\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    443\u001b[0m \n\u001b[0;32m    444\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    445\u001b[0m \u001b[38;5;66;03m# Initialize the plotter object\u001b[39;00m\n\u001b[1;32m--> 446\u001b[0m plotter \u001b[38;5;241m=\u001b[39m \u001b[43m_HeatMapper\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvmin\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvmax\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcmap\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcenter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrobust\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mannot\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfmt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    447\u001b[0m \u001b[43m                      \u001b[49m\u001b[43mannot_kws\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcbar\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcbar_kws\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mxticklabels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    448\u001b[0m \u001b[43m                      \u001b[49m\u001b[43myticklabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    450\u001b[0m \u001b[38;5;66;03m# Add the pcolormesh kwargs here\u001b[39;00m\n\u001b[0;32m    451\u001b[0m kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlinewidths\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m linewidths\n",
      "File \u001b[1;32mc:\\Users\\javer\\Anaconda3\\envs\\pymc5_env\\Lib\\site-packages\\seaborn\\matrix.py:110\u001b[0m, in \u001b[0;36m_HeatMapper.__init__\u001b[1;34m(self, data, vmin, vmax, cmap, center, robust, annot, fmt, annot_kws, cbar, cbar_kws, xticklabels, yticklabels, mask)\u001b[0m\n\u001b[0;32m    108\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    109\u001b[0m     plot_data \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray(data)\n\u001b[1;32m--> 110\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDataFrame\u001b[49m\u001b[43m(\u001b[49m\u001b[43mplot_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    112\u001b[0m \u001b[38;5;66;03m# Validate the mask and convert to DataFrame\u001b[39;00m\n\u001b[0;32m    113\u001b[0m mask \u001b[38;5;241m=\u001b[39m _matrix_mask(data, mask)\n",
      "File \u001b[1;32mc:\\Users\\javer\\Anaconda3\\envs\\pymc5_env\\Lib\\site-packages\\pandas\\core\\frame.py:785\u001b[0m, in \u001b[0;36mDataFrame.__init__\u001b[1;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[0;32m    774\u001b[0m         mgr \u001b[38;5;241m=\u001b[39m dict_to_mgr(\n\u001b[0;32m    775\u001b[0m             \u001b[38;5;66;03m# error: Item \"ndarray\" of \"Union[ndarray, Series, Index]\" has no\u001b[39;00m\n\u001b[0;32m    776\u001b[0m             \u001b[38;5;66;03m# attribute \"name\"\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    782\u001b[0m             copy\u001b[38;5;241m=\u001b[39m_copy,\n\u001b[0;32m    783\u001b[0m         )\n\u001b[0;32m    784\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 785\u001b[0m         mgr \u001b[38;5;241m=\u001b[39m \u001b[43mndarray_to_mgr\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    786\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    787\u001b[0m \u001b[43m            \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    788\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    789\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    790\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    791\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtyp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmanager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    792\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    794\u001b[0m \u001b[38;5;66;03m# For data is list-like, or Iterable (will consume into list)\u001b[39;00m\n\u001b[0;32m    795\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m is_list_like(data):\n",
      "File \u001b[1;32mc:\\Users\\javer\\Anaconda3\\envs\\pymc5_env\\Lib\\site-packages\\pandas\\core\\internals\\construction.py:314\u001b[0m, in \u001b[0;36mndarray_to_mgr\u001b[1;34m(values, index, columns, dtype, copy, typ)\u001b[0m\n\u001b[0;32m    308\u001b[0m     _copy \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    309\u001b[0m         copy_on_sanitize\n\u001b[0;32m    310\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m (dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m astype_is_view(values\u001b[38;5;241m.\u001b[39mdtype, dtype))\n\u001b[0;32m    311\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    312\u001b[0m     )\n\u001b[0;32m    313\u001b[0m     values \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(values, copy\u001b[38;5;241m=\u001b[39m_copy)\n\u001b[1;32m--> 314\u001b[0m     values \u001b[38;5;241m=\u001b[39m \u001b[43m_ensure_2d\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    316\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    317\u001b[0m     \u001b[38;5;66;03m# by definition an array here\u001b[39;00m\n\u001b[0;32m    318\u001b[0m     \u001b[38;5;66;03m# the dtypes will be coerced to a single dtype\u001b[39;00m\n\u001b[0;32m    319\u001b[0m     values \u001b[38;5;241m=\u001b[39m _prep_ndarraylike(values, copy\u001b[38;5;241m=\u001b[39mcopy_on_sanitize)\n",
      "File \u001b[1;32mc:\\Users\\javer\\Anaconda3\\envs\\pymc5_env\\Lib\\site-packages\\pandas\\core\\internals\\construction.py:592\u001b[0m, in \u001b[0;36m_ensure_2d\u001b[1;34m(values)\u001b[0m\n\u001b[0;32m    590\u001b[0m     values \u001b[38;5;241m=\u001b[39m values\u001b[38;5;241m.\u001b[39mreshape((values\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;241m1\u001b[39m))\n\u001b[0;32m    591\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m values\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[1;32m--> 592\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMust pass 2-d input. shape=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvalues\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    593\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m values\n",
      "\u001b[1;31mValueError\u001b[0m: Must pass 2-d input. shape=(2, 50, 50)"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x800 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sentencepiece as spm\n",
    "import torch\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "# Load the trained tokenizer\n",
    "tokenizer = spm.SentencePieceProcessor()\n",
    "tokenizer.load(\"urdu_english_bpe_parallel.model\")\n",
    "\n",
    "# Define max sequence length and pad index\n",
    "MAX_SEQ_LEN = 50\n",
    "PAD_IDX = tokenizer.piece_to_id(\"<pad>\")\n",
    "\n",
    "d_model = 128\n",
    "num_heads = 2\n",
    "num_layers = 2\n",
    "d_ff = 512\n",
    "max_len = 200\n",
    "dropout = 0.1\n",
    "\n",
    "src_vocab_size = tokenizer.get_piece_size()\n",
    "tgt_vocab_size = tokenizer.get_piece_size()\n",
    "\n",
    "\n",
    "# Create the model\n",
    "model1 = TransformerModel(src_vocab_size, tgt_vocab_size, PAD_IDX, PAD_IDX,\n",
    "                         d_model, num_heads, num_layers, d_ff, max_len, dropout)\n",
    "\n",
    "# Move the model to the appropriate device (e.g., GPU if available)\n",
    "model1 = model1.to(device)\n",
    "\n",
    "def plot_attention_weights(src_sentence, tgt_sentence, attention_weights, tokenizer):\n",
    "    # Tokenize the sentences\n",
    "    src_tokens = tokenizer.encode(src_sentence, out_type=str)\n",
    "    tgt_tokens = tokenizer.encode(tgt_sentence, out_type=str)\n",
    "    \n",
    "    # Normalize attention weights for visualization\n",
    "    attention_weights = attention_weights.squeeze().cpu().numpy()\n",
    "    \n",
    "    # Plot the attention weights\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(attention_weights, xticklabels=src_tokens, yticklabels=tgt_tokens,\n",
    "                cmap='Blues', cbar=True, annot=False)\n",
    "    plt.xlabel('Source (English) words')\n",
    "    plt.ylabel('Target (Urdu) words')\n",
    "    plt.title('Attention Visualization')\n",
    "    plt.show()\n",
    "\n",
    "def visualize_attention(model1, src_sentence, tgt_sentence, tokenizer, device):\n",
    "    # Convert source and target sentence to tensors\n",
    "    src_tensor = torch.tensor(tokenize_and_prepare([src_sentence]), dtype=torch.long).to(device)\n",
    "    tgt_tensor = torch.tensor(tokenize_and_prepare([tgt_sentence]), dtype=torch.long).to(device)\n",
    "\n",
    "    # Get the model output and attention weights\n",
    "    with torch.no_grad():\n",
    "        transformer_output, attention_weights = model1(src_tensor, tgt_tensor)\n",
    "\n",
    "    # Unpack attention weights\n",
    "    enc_attn_weights, dec_self_attn_weights, dec_enc_dec_attn_weights = attention_weights\n",
    "\n",
    "    # Assuming you're visualizing the first layer's attention (adjust index as needed)\n",
    "    # Decoding attention weights, change the indices to visualize different layers or heads\n",
    "    layer_idx = 0  # Choose layer index (first layer)\n",
    "    head_idx = 0   # Choose attention head index (first head)\n",
    "\n",
    "    plot_attention_weights(src_sentence, tgt_sentence, dec_enc_dec_attn_weights[layer_idx][head_idx], tokenizer)\n",
    "\n",
    "# Example usage\n",
    "src_sentence = \"This is an example sentence.\"\n",
    "tgt_sentence = \"یہ ایک مثال جملہ ہے۔\"\n",
    "\n",
    "# Assuming you have a trained transformer model loaded as `model1`\n",
    "visualize_attention(model1, src_sentence, tgt_sentence, tokenizer, device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=\"seagreen\"> **Fine Tuning Pretrained Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install transformers datasets torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install sacremoses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install torch==2.1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.1.0+cpu\n",
      "Accelerate version: 1.1.1\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import accelerate\n",
    "\n",
    "print(\"PyTorch version:\", torch.__version__)\n",
    "print(\"Accelerate version:\", accelerate.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip uninstall torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install torchvision==0.15.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\javer\\Anaconda3\\envs\\pymc5_env\\Lib\\site-packages\\torch\\_utils.py:776: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    }
   ],
   "source": [
    "from transformers import MarianMTModel, MarianTokenizer\n",
    "\n",
    "# Load the pretrained MarianMT model for English-Urdu\n",
    "model_name = \"Helsinki-NLP/opus-mt-en-ur\"\n",
    "model = MarianMTModel.from_pretrained(model_name)\n",
    "tokenizer = MarianTokenizer.from_pretrained(model_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "590bb63b608143efaafade12c21a5474",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "# Assume you have a validation dataset (for example, you can split your data)\n",
    "validation_data = Dataset.from_dict({\n",
    "    \"en\": en_sentences[:1000],  # Use a subset of sentences for validation\n",
    "    \"ur\": ur_sentences[:1000]\n",
    "})\n",
    "\n",
    "# Tokenize the validation data\n",
    "tokenized_validation_data = validation_data.map(tokenize_function, batched=True)\n",
    "\n",
    "# Now, pass this dataset as the `eval_dataset`\n",
    "trainer = Trainer(\n",
    "    model=model,  # Pre-trained model to fine-tune\n",
    "    args=training_args,  # Training arguments\n",
    "    train_dataset=tokenized_dataset,  # Tokenized training dataset\n",
    "    eval_dataset=tokenized_validation_data  # Validation dataset for evaluation\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4f67b7938184befa95e14290bfc1152",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\javer\\Anaconda3\\envs\\pymc5_env\\Lib\\site-packages\\huggingface_hub\\file_download.py:157: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\javer\\.cache\\huggingface\\hub\\models--roberta-base. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7427810eb3fb43639e5e4598c08849a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eee34ad504574fd7876df377773da9aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8bbd032b7e44467da3ad27223aba0a1b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3be816ba6dc4c75a66e615772e32b6a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/481 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f40f3c4c9e545d6bcbd7ca4f093a7d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['en', 'ur', 'input_ids', 'attention_mask', 'labels'],\n",
      "    num_rows: 2\n",
      "})\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "412d94a548ff49d4b48a49f365a35c53",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\javer\\Anaconda3\\envs\\pymc5_env\\Lib\\site-packages\\transformers\\modeling_utils.py:2817: UserWarning: Moving the following attributes in the config to the generation config: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[62024]]}. You are seeing this warning because you've set generation parameters in the model config, as opposed to in the generation config.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 5.416, 'train_samples_per_second': 1.108, 'train_steps_per_second': 0.554, 'train_loss': 11.561149597167969, 'epoch': 3.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=3, training_loss=11.561149597167969, metrics={'train_runtime': 5.416, 'train_samples_per_second': 1.108, 'train_steps_per_second': 0.554, 'total_flos': 9533915136.0, 'train_loss': 11.561149597167969, 'epoch': 3.0})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "from transformers import RobertaTokenizer  # Replace with your specific tokenizer\n",
    "\n",
    "# Initialize your tokenizer (replace with SentencePiece tokenizer if needed)\n",
    "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')  # Or use the tokenizer of your model\n",
    "\n",
    "# Sample data\n",
    "en_sentences = [\"Hello world.\", \"How are you?\"]\n",
    "ur_sentences = [\"سلام دنیا\", \"آپ کیسے ہیں؟\"]\n",
    "\n",
    "# Create a dataset\n",
    "data = {\n",
    "    \"en\": en_sentences,\n",
    "    \"ur\": ur_sentences\n",
    "}\n",
    "\n",
    "dataset = Dataset.from_dict(data)\n",
    "\n",
    "# Tokenizer function with padding\n",
    "def tokenize_function(examples):\n",
    "    # Tokenize both the source (en) and target (ur) sentences\n",
    "    inputs = tokenizer(examples['en'], padding=True, truncation=True, max_length=128, return_tensors=\"pt\")\n",
    "    targets = tokenizer(examples['ur'], padding=True, truncation=True, max_length=128, return_tensors=\"pt\")\n",
    "    \n",
    "    # Return tokenized data in the expected format\n",
    "    return {\n",
    "        'input_ids': inputs['input_ids'].squeeze(),\n",
    "        'attention_mask': inputs['attention_mask'].squeeze(),\n",
    "        'labels': targets['input_ids'].squeeze()\n",
    "    }\n",
    "\n",
    "# Apply tokenization to the dataset\n",
    "tokenized_dataset = dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# Check the dataset\n",
    "print(tokenized_dataset)\n",
    "\n",
    "# Set up training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    evaluation_strategy=\"no\",  # Disable evaluation if you don't have a validation set\n",
    "    learning_rate=5e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    num_train_epochs=3,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=100\n",
    ")\n",
    "\n",
    "# Set up the Trainer with the properly formatted dataset\n",
    "trainer = Trainer(\n",
    "    model=model,  # Pre-trained model to fine-tune\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset,  # Tokenized dataset\n",
    "    eval_dataset=None  # No validation dataset for now\n",
    ")\n",
    "\n",
    "# Fine-tune the model\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./model\\\\tokenizer_config.json',\n",
       " './model\\\\special_tokens_map.json',\n",
       " './model\\\\vocab.json',\n",
       " './model\\\\merges.txt',\n",
       " './model\\\\added_tokens.json')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.save_pretrained('./model')  # Save the model\n",
    "tokenizer.save_pretrained('./model')  # Save the tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\javer\\Anaconda3\\envs\\pymc5_env\\Lib\\site-packages\\torch\\_utils.py:776: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "602f40a8aa084a03b24ed5cf65c65387",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09b2e9f2866445c3821b2afed44fb4b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46bb9bb4b8f942c991972a674dffa378",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 5.557419300079346, 'eval_runtime': 0.095, 'eval_samples_per_second': 21.052, 'eval_steps_per_second': 10.526, 'epoch': 1.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95ab26b2f852400085561c08ad6dd1c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 4.130854606628418, 'eval_runtime': 0.0983, 'eval_samples_per_second': 20.35, 'eval_steps_per_second': 10.175, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\javer\\Anaconda3\\envs\\pymc5_env\\Lib\\site-packages\\transformers\\modeling_utils.py:2817: UserWarning: Moving the following attributes in the config to the generation config: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[62024]]}. You are seeing this warning because you've set generation parameters in the model config, as opposed to in the generation config.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01832abf5ede42658052b88708ad610c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 3.5496418476104736, 'eval_runtime': 0.1126, 'eval_samples_per_second': 17.769, 'eval_steps_per_second': 8.885, 'epoch': 3.0}\n",
      "{'train_runtime': 5.2598, 'train_samples_per_second': 1.141, 'train_steps_per_second': 0.57, 'train_loss': 6.180091222127278, 'epoch': 3.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "caaa4a3902d34dad8de4a8593af42622",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 3.5496418476104736, 'eval_runtime': 0.0995, 'eval_samples_per_second': 20.097, 'eval_steps_per_second': 10.049, 'epoch': 3.0}\n"
     ]
    }
   ],
   "source": [
    "from transformers import MarianMTModel, MarianTokenizer, Trainer, TrainingArguments\n",
    "from datasets import Dataset\n",
    "\n",
    "# Load the pretrained MarianMT model for English-Urdu\n",
    "model_name = \"Helsinki-NLP/opus-mt-en-ur\"\n",
    "model = MarianMTModel.from_pretrained(model_name)\n",
    "tokenizer = MarianTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Sample data (for the sake of example)\n",
    "en_sentences = [\"Hello world.\", \"How are you?\"]\n",
    "ur_sentences = [\"سلام دنیا\", \"آپ کیسے ہیں؟\"]\n",
    "\n",
    "# Create a dataset\n",
    "data = {\n",
    "    \"en\": en_sentences,\n",
    "    \"ur\": ur_sentences\n",
    "}\n",
    "\n",
    "# Create dataset from the data\n",
    "dataset = Dataset.from_dict(data)\n",
    "\n",
    "# Tokenization function\n",
    "def tokenize_function(examples):\n",
    "    # Tokenize both the source (en) and target (ur) sentences\n",
    "    inputs = tokenizer(examples['en'], padding=True, truncation=True, max_length=128, return_tensors=\"pt\")\n",
    "    targets = tokenizer(examples['ur'], padding=True, truncation=True, max_length=128, return_tensors=\"pt\")\n",
    "    \n",
    "    # Return tokenized data with labels (target is used as 'labels')\n",
    "    return {\n",
    "        'input_ids': inputs['input_ids'].squeeze(),\n",
    "        'attention_mask': inputs['attention_mask'].squeeze(),\n",
    "        'labels': targets['input_ids'].squeeze()  # Add target as labels\n",
    "    }\n",
    "\n",
    "# Apply tokenization to the dataset\n",
    "tokenized_dataset = dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    evaluation_strategy=\"epoch\",  # For evaluation during training\n",
    "    learning_rate=5e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    num_train_epochs=3,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=100\n",
    ")\n",
    "\n",
    "# Initialize Trainer with the dataset and training arguments\n",
    "trainer = Trainer(\n",
    "    model=model,  # Pre-trained model to fine-tune\n",
    "    args=training_args,  # Training arguments\n",
    "    train_dataset=tokenized_dataset,  # Tokenized training dataset\n",
    "    eval_dataset=tokenized_dataset  # Tokenized validation dataset (for this example, using the same)\n",
    ")\n",
    "\n",
    "# Fine-tune the model\n",
    "trainer.train()\n",
    "\n",
    "# Save the model and tokenizer\n",
    "model.save_pretrained('./model')\n",
    "tokenizer.save_pretrained('./model')\n",
    "\n",
    "# Optionally, evaluate the model\n",
    "eval_results = trainer.evaluate()\n",
    "print(eval_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.7006701231002808\n",
      "Epoch 2, Loss: 0.5639789700508118\n",
      "Epoch 3, Loss: 0.41531068086624146\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import BertForSequenceClassification, BertTokenizer\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch\n",
    "\n",
    "# Load pretrained model and tokenizer\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Example dataset class\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_length):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        text = self.texts[item]\n",
    "        label = self.labels[item]\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            text, \n",
    "            add_special_tokens=True, \n",
    "            max_length=self.max_length, \n",
    "            padding='max_length', \n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'labels': torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "# Sample data\n",
    "texts = [\"I love machine learning\", \"I hate bugs\"]\n",
    "labels = [1, 0]  # 1 = positive, 0 = negative\n",
    "\n",
    "# Prepare DataLoader\n",
    "dataset = MyDataset(texts, labels, tokenizer, max_length=32)\n",
    "train_loader = DataLoader(dataset, batch_size=2)\n",
    "\n",
    "# Optimizer and loss function\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=2e-5)\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# Training loop\n",
    "model.train()\n",
    "for epoch in range(3):\n",
    "    for batch in train_loader:\n",
    "        input_ids = batch['input_ids']\n",
    "        attention_mask = batch['attention_mask']\n",
    "        labels = batch['labels']\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        logits = outputs.logits\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        print(f\"Epoch {epoch + 1}, Loss: {loss.item()}\")\n",
    "\n",
    "# Evaluation\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=\"seagreen\"> **Comparing Results to Custom Transformers**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Transformer(\n",
       "  (encoder): Encoder(\n",
       "    (embedding): Embedding(32000, 128)\n",
       "    (pos_encoding): PositionalEncoding()\n",
       "    (layers): ModuleList(\n",
       "      (0-1): 2 x EncoderLayer(\n",
       "        (self_attn): MultiHeadAttention(\n",
       "          (q_linear): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (k_linear): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (v_linear): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (out): Linear(in_features=128, out_features=128, bias=True)\n",
       "        )\n",
       "        (ff): FeedForward(\n",
       "          (linear1): Linear(in_features=128, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): Linear(in_features=512, out_features=128, bias=True)\n",
       "        )\n",
       "        (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (embedding): Embedding(32000, 128)\n",
       "    (pos_encoding): PositionalEncoding()\n",
       "    (layers): ModuleList(\n",
       "      (0-1): 2 x DecoderLayer(\n",
       "        (self_attn): MultiHeadAttention(\n",
       "          (q_linear): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (k_linear): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (v_linear): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (out): Linear(in_features=128, out_features=128, bias=True)\n",
       "        )\n",
       "        (enc_dec_attn): MultiHeadAttention(\n",
       "          (q_linear): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (k_linear): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (v_linear): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (out): Linear(in_features=128, out_features=128, bias=True)\n",
       "        )\n",
       "        (ff): FeedForward(\n",
       "          (linear1): Linear(in_features=128, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): Linear(in_features=512, out_features=128, bias=True)\n",
       "        )\n",
       "        (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm3): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (fc_out): Linear(in_features=128, out_features=32000, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "# Load the trained tokenizer\n",
    "custom_tokenizer = spm.SentencePieceProcessor()\n",
    "custom_tokenizer.load(\"urdu_english_bpe_parallel.model\")\n",
    "\n",
    "\n",
    "# Define model hyperparameters\n",
    "d_model = 128\n",
    "num_heads = 2\n",
    "num_layers = 2\n",
    "d_ff = 512\n",
    "max_len = 200\n",
    "dropout = 0.1\n",
    "\n",
    "src_vocab_size = custom_tokenizer.get_piece_size()\n",
    "tgt_vocab_size = custom_tokenizer.get_piece_size()\n",
    "\n",
    "    \n",
    "# Initialize the Transformer model\n",
    "# Assuming PAD_IDX is already defined, make sure both are passed:\n",
    "custom_model = Transformer(\n",
    "    src_vocab_size=src_vocab_size,\n",
    "    tgt_vocab_size=tgt_vocab_size,\n",
    "    src_pad_idx=PAD_IDX,  # src padding index\n",
    "    tgt_pad_idx=PAD_IDX,  # tgt padding index\n",
    "    d_model=d_model,\n",
    "    num_heads=num_heads,\n",
    "    num_layers=num_layers,\n",
    "    d_ff=d_ff,\n",
    "    max_len=max_len,\n",
    "    dropout=dropout\n",
    ")\n",
    "\n",
    "\n",
    "custom_model.load_state_dict(torch.load('transformer_translation1.pth'))\n",
    "custom_model.to(device)\n",
    "custom_model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Transformer' object has no attribute 'generate'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[64], line 51\u001b[0m\n\u001b[0;32m     48\u001b[0m input_sentences \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHello world.\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHow are you?\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m     50\u001b[0m \u001b[38;5;66;03m# Get translations from both models\u001b[39;00m\n\u001b[1;32m---> 51\u001b[0m marian_translations \u001b[38;5;241m=\u001b[39m \u001b[43mtranslate_with_marian_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_sentences\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     52\u001b[0m custom_translations \u001b[38;5;241m=\u001b[39m translate_with_custom_model(input_sentences)\n\u001b[0;32m     54\u001b[0m \u001b[38;5;66;03m# Print the results for comparison\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[64], line 14\u001b[0m, in \u001b[0;36mtranslate_with_marian_model\u001b[1;34m(input_texts)\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# Perform translation using Marian's generate method\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m---> 14\u001b[0m     translated \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m(inputs)  \u001b[38;5;66;03m# Assuming model.generate() takes 'input_ids' only\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m# Decode the translated text\u001b[39;00m\n\u001b[0;32m     17\u001b[0m translated_texts \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mdecode(translated[\u001b[38;5;241m0\u001b[39m], skip_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\javer\\Anaconda3\\envs\\pymc5_env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1614\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   1612\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m modules:\n\u001b[0;32m   1613\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m modules[name]\n\u001b[1;32m-> 1614\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m   1615\u001b[0m     \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, name))\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'Transformer' object has no attribute 'generate'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "device = 'cpu'\n",
    "\n",
    "# Define a function to translate with the pretrained Marian model\n",
    "def translate_with_marian_model(input_texts):\n",
    "    # Tokenize input texts (each text individually)\n",
    "    inputs = tokenizer.encode(input_texts, out_type=int)  # returns a list of token IDs\n",
    "    inputs = torch.tensor(inputs).unsqueeze(0).to(device)  # Convert to tensor and add batch dimension\n",
    "    \n",
    "    # Perform translation using Marian's generate method\n",
    "    with torch.no_grad():\n",
    "        translated = model.generate(inputs)  # Assuming model.generate() takes 'input_ids' only\n",
    "    \n",
    "    # Decode the translated text\n",
    "    translated_texts = tokenizer.decode(translated[0], skip_special_tokens=True)\n",
    "    return translated_texts\n",
    "\n",
    "\n",
    "# Define a function to translate with the custom model (greedy decoding)\n",
    "def translate_with_custom_model(input_texts):\n",
    "    # Tokenize input texts (each text individually)\n",
    "    input_ids = custom_tokenizer.encode(input_texts, out_type=int)  # returns a list of token IDs\n",
    "    input_ids = torch.tensor(input_ids).unsqueeze(0).to(device)  # Convert to tensor and add batch dimension\n",
    "    \n",
    "    # Start with the input IDs and generate the translation\n",
    "    generated_ids = input_ids  # Start with input IDs\n",
    "    with torch.no_grad():\n",
    "        for _ in range(100):  # Limiting the number of tokens to generate\n",
    "            outputs = custom_model(generated_ids)  # Assuming your custom model returns logits\n",
    "            logits = outputs[0, -1, :]  # Get the logits for the last token\n",
    "            \n",
    "            # Get the token with the highest probability\n",
    "            next_token_id = logits.argmax(dim=-1).unsqueeze(0).unsqueeze(0)  # Add batch and sequence dims\n",
    "            generated_ids = torch.cat((generated_ids, next_token_id), dim=1)  # Append to the sequence\n",
    "            \n",
    "            # Check if end-of-sequence token is generated (adjust token ID accordingly)\n",
    "            if next_token_id.item() == custom_tokenizer.piece_to_id('<eos>'):  # Assuming '<eos>' is end token\n",
    "                break\n",
    "    \n",
    "    # Decode the translated text\n",
    "    translated_texts = custom_tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
    "    return translated_texts\n",
    "\n",
    "\n",
    "# Example input sentences\n",
    "input_sentences = [\"Hello world.\", \"How are you?\"]\n",
    "\n",
    "# Get translations from both models\n",
    "marian_translations = translate_with_marian_model(input_sentences)\n",
    "custom_translations = translate_with_custom_model(input_sentences)\n",
    "\n",
    "# Print the results for comparison\n",
    "print(\"Marian Model Translations:\", marian_translations)\n",
    "print(\"Custom Transformer Model Translations:\", custom_translations)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'custom_translations' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[65], line 7\u001b[0m\n\u001b[0;32m      5\u001b[0m references \u001b[38;5;241m=\u001b[39m [[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mسلام دنیا\u001b[39m\u001b[38;5;124m'\u001b[39m], [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mآپ کیسے ہیں؟\u001b[39m\u001b[38;5;124m'\u001b[39m]]  \u001b[38;5;66;03m# Actual translations in Urdu\u001b[39;00m\n\u001b[0;32m      6\u001b[0m candidates_marian \u001b[38;5;241m=\u001b[39m [marian_translations\u001b[38;5;241m.\u001b[39msplit()]\n\u001b[1;32m----> 7\u001b[0m candidates_custom \u001b[38;5;241m=\u001b[39m [\u001b[43mcustom_translations\u001b[49m\u001b[38;5;241m.\u001b[39msplit()]\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# Compute BLEU scores for both models\u001b[39;00m\n\u001b[0;32m     10\u001b[0m bleu_marian \u001b[38;5;241m=\u001b[39m corpus_bleu(references, candidates_marian)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'custom_translations' is not defined"
     ]
    }
   ],
   "source": [
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "\n",
    "# Example: List of reference translations (ground truth) and candidate translations\n",
    "# Assume reference translations are a list of lists of tokens (since BLEU works on tokenized data)\n",
    "references = [['سلام دنیا'], ['آپ کیسے ہیں؟']]  # Actual translations in Urdu\n",
    "candidates_marian = [marian_translations.split()]\n",
    "candidates_custom = [custom_translations.split()]\n",
    "\n",
    "# Compute BLEU scores for both models\n",
    "bleu_marian = corpus_bleu(references, candidates_marian)\n",
    "bleu_custom = corpus_bleu(references, candidates_custom)\n",
    "\n",
    "print(f\"BLEU score for Marian Model: {bleu_marian}\")\n",
    "print(f\"BLEU score for Custom Model: {bleu_custom}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'to'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[66], line 27\u001b[0m\n\u001b[0;32m     24\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m total_loss \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(eval_dataloader)\n\u001b[0;32m     26\u001b[0m \u001b[38;5;66;03m# Calculate the losses\u001b[39;00m\n\u001b[1;32m---> 27\u001b[0m loss_custom \u001b[38;5;241m=\u001b[39m \u001b[43mcompute_loss_custom_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     28\u001b[0m loss_marian \u001b[38;5;241m=\u001b[39m compute_loss_marian_model()\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoss for Custom Model: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss_custom\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[66], line 10\u001b[0m, in \u001b[0;36mcompute_loss_custom_model\u001b[1;34m()\u001b[0m\n\u001b[0;32m      8\u001b[0m total_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m eval_dataloader:\n\u001b[1;32m---> 10\u001b[0m     input_ids \u001b[38;5;241m=\u001b[39m \u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43minput_ids\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m(device)\n\u001b[0;32m     11\u001b[0m     labels \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     12\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m custom_model(input_ids, labels\u001b[38;5;241m=\u001b[39mlabels)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'list' object has no attribute 'to'"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Define a DataLoader for your validation dataset\n",
    "eval_dataloader = DataLoader(tokenized_eval_dataset, batch_size=32)\n",
    "\n",
    "# Function to compute loss for custom model\n",
    "def compute_loss_custom_model():\n",
    "    total_loss = 0\n",
    "    for batch in eval_dataloader:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        outputs = custom_model(input_ids, labels=labels)\n",
    "        total_loss += outputs.loss.item()  # Assuming the model returns loss\n",
    "    return total_loss / len(eval_dataloader)\n",
    "\n",
    "# Function to compute loss for Marian model\n",
    "def compute_loss_marian_model():\n",
    "    total_loss = 0\n",
    "    for batch in eval_dataloader:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        outputs = model(input_ids, labels=labels)\n",
    "        total_loss += outputs.loss.item()  # Marian returns loss directly\n",
    "    return total_loss / len(eval_dataloader)\n",
    "\n",
    "# Calculate the losses\n",
    "loss_custom = compute_loss_custom_model()\n",
    "loss_marian = compute_loss_marian_model()\n",
    "\n",
    "print(f\"Loss for Custom Model: {loss_custom}\")\n",
    "print(f\"Loss for Marian Model: {loss_marian}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pymc5_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
