{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question # 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Preprocessing & Tokenizing `(Byte-Pair Encoding)` Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Set the main directory for datasets\n",
    "dataset_root = \"umc005-corpus\"\n",
    "quran_folder = os.path.join(dataset_root, \"quran\")\n",
    "bible_folder = os.path.join(dataset_root, \"bible\")\n",
    "\n",
    "# Helper function to read aligned sentence pairs from file paths\n",
    "def read_parallel_corpus(english_path, urdu_path):\n",
    "    with open(english_path, \"r\", encoding=\"utf-8\") as file_en, open(urdu_path, \"r\", encoding=\"utf-8\") as file_ur:\n",
    "        en_lines = file_en.readlines()\n",
    "        ur_lines = file_ur.readlines()\n",
    "\n",
    "    assert len(en_lines) == len(ur_lines), \"Line count mismatch between English and Urdu files.\"\n",
    "    paired_sentences = list(zip(en_lines, ur_lines))\n",
    "    return paired_sentences\n",
    "\n",
    "# Load Quran dataset\n",
    "train_quran = read_parallel_corpus(os.path.join(quran_folder, \"train.en\"), os.path.join(quran_folder, \"train.ur\"))\n",
    "test_quran = read_parallel_corpus(os.path.join(quran_folder, \"test.en\"), os.path.join(quran_folder, \"test.ur\"))\n",
    "dev_quran = read_parallel_corpus(os.path.join(quran_folder, \"dev.en\"), os.path.join(quran_folder, \"dev.ur\"))\n",
    "\n",
    "# Load Bible dataset\n",
    "train_bible = read_parallel_corpus(os.path.join(bible_folder, \"train.en\"), os.path.join(bible_folder, \"train.ur\"))\n",
    "test_bible = read_parallel_corpus(os.path.join(bible_folder, \"test.en\"), os.path.join(bible_folder, \"test.ur\"))\n",
    "dev_bible = read_parallel_corpus(os.path.join(bible_folder, \"dev.en\"), os.path.join(bible_folder, \"dev.ur\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized text: ['▁', 'S', 'am', 'ple', '▁te', 'xt', '▁in', '▁', 'E', 'ng', 'lish', '▁or', '▁', 'U', 'rd', 'u', '▁for', '▁token', 'iz', 'ation']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sentencepiece as spm\n",
    "\n",
    "# Function to clean English text\n",
    "def preprocess_english(text_line):\n",
    "    text_line = text_line.strip().lower()  # Convert to lowercase and trim spaces\n",
    "    return text_line\n",
    "\n",
    "# Define main directories for UMC005 dataset\n",
    "corpus_root = \"umc005-corpus\"\n",
    "quran_path = os.path.join(corpus_root, \"quran\")\n",
    "bible_path = os.path.join(corpus_root, \"bible\")\n",
    "\n",
    "# Function to read aligned data from English and Urdu files\n",
    "def read_aligned_sentences(path_en, path_ur):\n",
    "    with open(path_en, \"r\", encoding=\"utf-8\") as file_en, open(path_ur, \"r\", encoding=\"utf-8\") as file_ur:\n",
    "        lines_en = file_en.readlines()\n",
    "        lines_ur = file_ur.readlines()\n",
    "\n",
    "    assert len(lines_en) == len(lines_ur), \"The English and Urdu files have different line counts.\"\n",
    "    paired_data = list(zip(lines_en, lines_ur))\n",
    "    return paired_data\n",
    "\n",
    "# Function to aggregate all parallel data\n",
    "def aggregate_parallel_corpus():\n",
    "    aligned_pairs = []\n",
    "    quran_data = read_aligned_sentences(os.path.join(quran_path, \"Quran-EN\"), os.path.join(quran_path, \"Quran-UR-normalized\"))\n",
    "    bible_data = read_aligned_sentences(os.path.join(bible_path, \"Bible-EN\"), os.path.join(bible_path, \"Bible-UR-normalized\"))\n",
    "    aligned_pairs.extend(quran_data + bible_data)\n",
    "    return aligned_pairs\n",
    "\n",
    "# Function to generate a unified parallel data file\n",
    "def write_parallel_corpus_to_file(sentence_pairs, output_filename):\n",
    "    with open(output_filename, \"w\", encoding=\"utf-8\") as output_file:\n",
    "        for eng_sentence, ur_sentence in sentence_pairs:\n",
    "            processed_eng = preprocess_english(eng_sentence)  # Clean the English sentence\n",
    "            output_file.write(processed_eng + \"\\n\")  # Write cleaned English text\n",
    "            output_file.write(ur_sentence.strip() + \"\\n\")  # Write Urdu text without extra spaces\n",
    "\n",
    "# Prepare and save the combined dataset\n",
    "merged_data = aggregate_parallel_corpus()\n",
    "output_corpus_file = \"urdu_english_combined_parallel.txt\"\n",
    "write_parallel_corpus_to_file(merged_data, output_corpus_file)\n",
    "\n",
    "# Train the SentencePiece model on the combined dataset\n",
    "spm.SentencePieceTrainer.train(\n",
    "    input=output_corpus_file,\n",
    "    model_prefix=\"bpe_tokenizer_urdu_english\",\n",
    "    vocab_size=32000,  # Set desired vocabulary size\n",
    "    model_type=\"bpe\",  # Use Byte-Pair Encoding for tokenization\n",
    "    character_coverage=0.9995,  # High coverage to include all characters\n",
    "    input_sentence_size=50000,  # Limit sentences for faster training\n",
    "    shuffle_input_sentence=True\n",
    ")\n",
    "\n",
    "# Load the trained SentencePiece model\n",
    "tokenizer = spm.SentencePieceProcessor(model_file=\"bpe_tokenizer_urdu_english.model\")\n",
    "\n",
    "# Example of tokenizing a sample sentence\n",
    "test_sentence = \"Sample text in English or Urdu for tokenization\"\n",
    "tokenized_output = tokenizer.encode(test_sentence, out_type=str)\n",
    "print(\"Tokenized text:\", tokenized_output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "\n",
    "\n",
    "# Multi-Head Attention module\n",
    "class MultiHeadedAttention(nn.Module):\n",
    "    def __init__(self, model_dim, num_attention_heads):\n",
    "        super(MultiHeadedAttention, self).__init__()\n",
    "        assert model_dim % num_attention_heads == 0, \"model_dim must be divisible by num_attention_heads\"\n",
    "        self.head_dim = model_dim // num_attention_heads\n",
    "        self.num_heads = num_attention_heads\n",
    "\n",
    "        # Define linear layers for query, key, and value projections\n",
    "        self.linear_query = nn.Linear(model_dim, model_dim)\n",
    "        self.linear_key = nn.Linear(model_dim, model_dim)\n",
    "        self.linear_value = nn.Linear(model_dim, model_dim)\n",
    "        self.output_linear = nn.Linear(model_dim, model_dim)\n",
    "\n",
    "    def forward(self, queries, keys, values, attention_mask=None):\n",
    "        batch_size = queries.size(0)\n",
    "\n",
    "        # Project queries, keys, and values\n",
    "        queries = self.linear_query(queries).view(batch_size, -1, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        keys = self.linear_key(keys).view(batch_size, -1, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        values = self.linear_value(values).view(batch_size, -1, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "\n",
    "        # Compute attention scores with scaled dot-product attention\n",
    "        scaled_scores = torch.matmul(queries, keys.transpose(-2, -1)) / math.sqrt(self.head_dim)\n",
    "        if attention_mask is not None:\n",
    "            scaled_scores = scaled_scores.masked_fill(attention_mask == 0, float('-inf'))\n",
    "\n",
    "        # Apply softmax to get attention weights\n",
    "        attention_weights = F.softmax(scaled_scores, dim=-1)\n",
    "        attention_output = torch.matmul(attention_weights, values)\n",
    "\n",
    "        # Concatenate the attention output from all heads\n",
    "        concatenated_output = attention_output.transpose(1, 2).contiguous().view(batch_size, -1, self.num_heads * self.head_dim)\n",
    "        return self.output_linear(concatenated_output)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Positional Encoding module for adding temporal/spatial information\n",
    "class PositionEncoding(nn.Module):\n",
    "    def __init__(self, model_dim, max_sequence_length=5000):\n",
    "        super(PositionEncoding, self).__init__()\n",
    "        self.position_encodings = torch.zeros(max_sequence_length, model_dim)\n",
    "        pos_indices = torch.arange(0, max_sequence_length).unsqueeze(1)\n",
    "        frequency_term = torch.exp(torch.arange(0, model_dim, 2) * -(math.log(10000.0) / model_dim))\n",
    "\n",
    "        # Compute sine and cosine values for positional encodings\n",
    "        self.position_encodings[:, 0::2] = torch.sin(pos_indices * frequency_term)\n",
    "        self.position_encodings[:, 1::2] = torch.cos(pos_indices * frequency_term)\n",
    "        self.position_encodings = self.position_encodings.unsqueeze(0)\n",
    "\n",
    "    def forward(self, input_tensor):\n",
    "        seq_length = input_tensor.size(1)\n",
    "        # Add positional encodings to the input tensor\n",
    "        return input_tensor + self.position_encodings[:, :seq_length, :].to(input_tensor.device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Feed-Forward Network module used in Transformer\n",
    "class TransformerFeedForward(nn.Module):\n",
    "    def __init__(self, model_dim, ff_dim, dropout_rate=0.1):\n",
    "        super(TransformerFeedForward, self).__init__()\n",
    "        self.fc1 = nn.Linear(model_dim, ff_dim)\n",
    "        self.dropout_layer = nn.Dropout(dropout_rate)\n",
    "        self.fc2 = nn.Linear(ff_dim, model_dim)\n",
    "\n",
    "    def forward(self, input_tensor):\n",
    "        # Apply first linear layer, followed by ReLU activation and dropout, then second linear layer\n",
    "        intermediate_output = F.relu(self.fc1(input_tensor))\n",
    "        dropped_output = self.dropout_layer(intermediate_output)\n",
    "        final_output = self.fc2(dropped_output)\n",
    "        return final_output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformer Encoder Layer\n",
    "class TransformerEncoderLayer(nn.Module):\n",
    "    def __init__(self, model_dim, num_attention_heads, ff_dim, dropout_rate=0.1):\n",
    "        super(TransformerEncoderLayer, self).__init__()\n",
    "        self.self_attention = MultiHeadedAttention(model_dim, num_attention_heads)\n",
    "        self.feed_forward = TransformerFeedForward(model_dim, ff_dim, dropout_rate)\n",
    "        self.layer_norm1 = nn.LayerNorm(model_dim)\n",
    "        self.layer_norm2 = nn.LayerNorm(model_dim)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "    def forward(self, input_tensor, attention_mask=None):\n",
    "        # Self-Attention with residual connection and layer normalization\n",
    "        attention_output = self.self_attention(input_tensor, input_tensor, input_tensor, attention_mask)\n",
    "        normed_attention_output = self.layer_norm1(input_tensor + self.dropout(attention_output))\n",
    "\n",
    "        # Feed-Forward with residual connection and layer normalization\n",
    "        ff_output = self.feed_forward(normed_attention_output)\n",
    "        final_output = self.layer_norm2(normed_attention_output + self.dropout(ff_output))\n",
    "        return final_output\n",
    "\n",
    "# Transformer Encoder Module\n",
    "class TransformerEncoder(nn.Module):\n",
    "    def __init__(self, vocab_size, model_dim, num_attention_heads, num_encoder_layers, ff_dim, max_sequence_length, dropout_rate=0.1):\n",
    "        super(TransformerEncoder, self).__init__()\n",
    "        self.embedding_layer = nn.Embedding(vocab_size, model_dim)\n",
    "        self.position_encoding = PositionEncoding(model_dim, max_sequence_length)\n",
    "        self.encoder_layers = nn.ModuleList([\n",
    "            TransformerEncoderLayer(model_dim, num_attention_heads, ff_dim, dropout_rate)\n",
    "            for _ in range(num_encoder_layers)\n",
    "        ])\n",
    "\n",
    "    def forward(self, input_indices, attention_mask=None):\n",
    "        # Apply embedding and scale by the square root of model dimension\n",
    "        embeddings = self.embedding_layer(input_indices) * math.sqrt(self.embedding_layer.embedding_dim)\n",
    "        embeddings = self.position_encoding(embeddings)\n",
    "\n",
    "        # Pass through each encoder layer\n",
    "        for encoder_layer in self.encoder_layers:\n",
    "            embeddings = encoder_layer(embeddings, attention_mask)\n",
    "\n",
    "        return embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformer Decoder Layer\n",
    "class TransformerDecoderLayer(nn.Module):\n",
    "    def __init__(self, model_dim, num_attention_heads, ff_dim, dropout_rate=0.1):\n",
    "        super(TransformerDecoderLayer, self).__init__()\n",
    "        self.self_attention = MultiHeadedAttention(model_dim, num_attention_heads)\n",
    "        self.encoder_decoder_attention = MultiHeadedAttention(model_dim, num_attention_heads)\n",
    "        self.feed_forward = TransformerFeedForward(model_dim, ff_dim, dropout_rate)\n",
    "        self.layer_norm1 = nn.LayerNorm(model_dim)\n",
    "        self.layer_norm2 = nn.LayerNorm(model_dim)\n",
    "        self.layer_norm3 = nn.LayerNorm(model_dim)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "    def forward(self, target_tensor, encoder_output, source_mask=None, target_mask=None):\n",
    "        # Self-attention with residual connection and layer normalization\n",
    "        self_attention_output = self.self_attention(target_tensor, target_tensor, target_tensor, target_mask)\n",
    "        normed_self_attention_output = self.layer_norm1(target_tensor + self.dropout(self_attention_output))\n",
    "\n",
    "        # Encoder-decoder attention with residual connection and layer normalization\n",
    "        enc_dec_attention_output = self.encoder_decoder_attention(normed_self_attention_output, encoder_output, encoder_output, source_mask)\n",
    "        normed_enc_dec_output = self.layer_norm2(normed_self_attention_output + self.dropout(enc_dec_attention_output))\n",
    "\n",
    "        # Feed-forward network with residual connection and layer normalization\n",
    "        ff_output = self.feed_forward(normed_enc_dec_output)\n",
    "        final_output = self.layer_norm3(normed_enc_dec_output + self.dropout(ff_output))\n",
    "        return final_output\n",
    "\n",
    "# Transformer Decoder Module\n",
    "class TransformerDecoder(nn.Module):\n",
    "    def __init__(self, vocab_size, model_dim, num_attention_heads, num_decoder_layers, ff_dim, max_sequence_length, dropout_rate=0.1):\n",
    "        super(TransformerDecoder, self).__init__()\n",
    "        self.embedding_layer = nn.Embedding(vocab_size, model_dim)\n",
    "        self.position_encoding = PositionEncoding(model_dim, max_sequence_length)\n",
    "        self.decoder_layers = nn.ModuleList([\n",
    "            TransformerDecoderLayer(model_dim, num_attention_heads, ff_dim, dropout_rate)\n",
    "            for _ in range(num_decoder_layers)\n",
    "        ])\n",
    "\n",
    "    def forward(self, target_indices, encoder_output, source_mask=None, target_mask=None):\n",
    "        # Apply embedding and scale by the square root of model dimension\n",
    "        target_embeddings = self.embedding_layer(target_indices) * math.sqrt(self.embedding_layer.embedding_dim)\n",
    "        target_embeddings = self.position_encoding(target_embeddings)\n",
    "\n",
    "        # Pass through each decoder layer\n",
    "        for decoder_layer in self.decoder_layers:\n",
    "            target_embeddings = decoder_layer(target_embeddings, encoder_output, source_mask, target_mask)\n",
    "\n",
    "        return target_embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, src_vocab_size, tgt_vocab_size, src_pad_idx, tgt_pad_idx, d_model, num_heads, num_layers, d_ff, max_len, dropout=0.1):\n",
    "        super(Transformer, self).__init__()\n",
    "        self.src_pad_idx = src_pad_idx\n",
    "        self.tgt_pad_idx = tgt_pad_idx\n",
    "        self.encoder = Encoder(src_vocab_size, d_model, num_heads, num_layers, d_ff, max_len, dropout)\n",
    "        self.decoder = Decoder(tgt_vocab_size, d_model, num_heads, num_layers, d_ff, max_len, dropout)\n",
    "        self.fc_out = nn.Linear(d_model, tgt_vocab_size)\n",
    "\n",
    "    def make_src_mask(self, src):\n",
    "        # Create mask for source\n",
    "        return (src != self.src_pad_idx).unsqueeze(1).unsqueeze(2)\n",
    "\n",
    "    def make_tgt_mask(self, tgt):\n",
    "        # Create mask for target (with causal mask for decoding)\n",
    "        tgt_pad_mask = (tgt != self.tgt_pad_idx).unsqueeze(1).unsqueeze(2)\n",
    "        tgt_len = tgt.size(1)\n",
    "        tgt_subseq_mask = torch.tril(torch.ones((tgt_len, tgt_len), device=tgt.device)).bool()\n",
    "        return tgt_pad_mask & tgt_subseq_mask\n",
    "\n",
    "    def forward(self, src, tgt):\n",
    "        src_mask = self.make_src_mask(src)\n",
    "        tgt_mask = self.make_tgt_mask(tgt)\n",
    "        enc_output = self.encoder(src, src_mask)\n",
    "        dec_output = self.decoder(tgt, enc_output, src_mask, tgt_mask)\n",
    "        output = self.fc_out(dec_output)\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sentencepiece as spm\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "# Load the trained tokenizer\n",
    "sentence_tokenizer = spm.SentencePieceProcessor()\n",
    "sentence_tokenizer.load(\"urdu_english_bpe_parallel.model\")\n",
    "\n",
    "# Define max sequence length and padding index\n",
    "MAX_SEQUENCE_LENGTH = 50\n",
    "PADDING_INDEX = sentence_tokenizer.piece_to_id(\"<pad>\")\n",
    "\n",
    "# Function to tokenize and pad the sentences\n",
    "def tokenize_and_pad_sentences(sentences):\n",
    "    tokenized_sentences = [sentence_tokenizer.encode(sentence, out_type=int) for sentence in sentences]\n",
    "    padded_sentences = [\n",
    "        seq[:MAX_SEQUENCE_LENGTH] + [PADDING_INDEX] * (MAX_SEQUENCE_LENGTH - len(seq)) if len(seq) < MAX_SEQUENCE_LENGTH else seq[:MAX_SEQUENCE_LENGTH]\n",
    "        for seq in tokenized_sentences\n",
    "    ]\n",
    "    return padded_sentences\n",
    "\n",
    "# Function to load sentences from file\n",
    "def load_sentences_from_file(file_path):\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "        return file.readlines()\n",
    "\n",
    "# Load training data\n",
    "english_train_sentences = load_sentences_from_file(\"umc005-corpus/quran/train.en\") + load_sentences_from_file(\"umc005-corpus/bible/train.en\")\n",
    "urdu_train_sentences = load_sentences_from_file(\"umc005-corpus/quran/train.ur\") + load_sentences_from_file(\"umc005-corpus/bible/train.ur\")\n",
    "\n",
    "# Tokenize and prepare the data\n",
    "source_data = tokenize_and_pad_sentences(english_train_sentences)\n",
    "target_data = tokenize_and_pad_sentences(urdu_train_sentences)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Translation Dataset class for paired source and target data\n",
    "class ParallelTextDataset(Dataset):\n",
    "    def __init__(self, source_data, target_data):\n",
    "        self.source_data = source_data\n",
    "        self.target_data = target_data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.source_data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return torch.tensor(self.source_data[index]), torch.tensor(self.target_data[index])\n",
    "\n",
    "# Create the dataset and dataloader\n",
    "parallel_text_dataset = ParallelTextDataset(source_data, target_data)\n",
    "data_loader = DataLoader(parallel_text_dataset, batch_size=2, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyperparameters & Model Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model hyperparameters\n",
    "model_dim = 128\n",
    "num_attention_heads = 2\n",
    "num_encoder_decoder_layers = 2\n",
    "feed_forward_dim = 512\n",
    "max_sequence_length = 200\n",
    "dropout_rate = 0.1\n",
    "\n",
    "# Assuming the tokenizer is defined and PAD_IDX is already provided\n",
    "source_vocab_size = tokenizer.get_piece_size()\n",
    "target_vocab_size = tokenizer.get_piece_size()\n",
    "\n",
    "# Initialize the Transformer model\n",
    "transformer_model = TransformerModel(\n",
    "    source_vocab_size=source_vocab_size,\n",
    "    target_vocab_size=target_vocab_size,\n",
    "    source_pad_token_idx=PAD_IDX,  # Source padding index\n",
    "    target_pad_token_idx=PAD_IDX,  # Target padding index\n",
    "    model_dim=model_dim,\n",
    "    num_attention_heads=num_attention_heads,\n",
    "    num_layers=num_encoder_decoder_layers,\n",
    "    ff_dim=feed_forward_dim,\n",
    "    max_sequence_length=max_sequence_length,\n",
    "    dropout_rate=dropout_rate\n",
    ")\n",
    "\n",
    "# Define loss function with padding ignored\n",
    "loss_criterion = torch.nn.CrossEntropyLoss(ignore_index=PAD_IDX)\n",
    "\n",
    "# Optimizer\n",
    "optimizer = torch.optim.Adam(transformer_model.parameters(), lr=0.0001)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Training the custom transformer** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20, Loss: 5.847795763905369\n",
      "Epoch 2/20, Loss: 4.914523594290463\n",
      "Epoch 3/20, Loss: 4.5751000347600055\n",
      "Epoch 4/20, Loss: 4.352262926599873\n",
      "Epoch 5/20, Loss: 4.184005458959892\n",
      "Epoch 6/20, Loss: 4.042845732934439\n",
      "Epoch 7/20, Loss: 3.918032662388104\n",
      "Epoch 8/20, Loss: 3.8128471590867683\n",
      "Epoch 9/20, Loss: 3.7139017724457073\n",
      "Epoch 10/20, Loss: 3.621993897566155\n",
      "Epoch 11/20, Loss: 3.534025015350598\n",
      "Epoch 12/20, Loss: 3.4561936408325806\n",
      "Epoch 13/20, Loss: 3.383977654051425\n",
      "Epoch 14/20, Loss: 3.312921400176945\n",
      "Epoch 15/20, Loss: 3.2490196408353635\n",
      "Epoch 16/20, Loss: 3.184731990588245\n",
      "Epoch 17/20, Loss: 3.1271846369531615\n",
      "Epoch 18/20, Loss: 3.0695784538717414\n",
      "Epoch 19/20, Loss: 3.020777788633731\n",
      "Epoch 20/20, Loss: 2.9712336649378734\n"
     ]
    }
   ],
   "source": [
    "# Set device to GPU if available, otherwise use CPU\n",
    "training_device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Move the model to the appropriate device\n",
    "transformer_model.to(training_device)\n",
    "\n",
    "num_epochs = 20\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    transformer_model.train()\n",
    "    total_epoch_loss = 0\n",
    "\n",
    "    for source_batch, target_batch in dataloader:\n",
    "        # Move data to the appropriate device\n",
    "        source_input = source_batch.to(training_device)\n",
    "        target_input = target_batch[:, :-1].to(training_device)  # Exclude last token\n",
    "        target_output = target_batch[:, 1:].to(training_device)  # Exclude first token\n",
    "\n",
    "        # Forward pass\n",
    "        predicted_logits = transformer_model(source_input, target_input)\n",
    "\n",
    "        # Compute loss\n",
    "        loss = loss_criterion(predicted_logits.view(-1, predicted_logits.size(-1)), target_output.view(-1))\n",
    "        total_epoch_loss += loss.item()\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {total_epoch_loss / len(dataloader)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(transformer_model.state_dict(), \"transformer_translation1.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translation: باتیں ہیں ہیں ہیں ہیں ہیں ہیں ہیں ہیں ہیں ہیں ہیں ہیں ہیں ہیں ہیں ہیں ہیں ہیں ہیں ہیں ہیں ہیں ہیں ہیں ہیں ہیں ہیں ہیں ہیں ہیں ہیں ہیں ہیں ہیں ہیں ہیں ہیں ہیں ہیں ہیں ہیں ہیں ہیں ہیں ہیں ہیں ہیں ہیں ہیں\n"
     ]
    }
   ],
   "source": [
    "def translate(transformer_model, tokenizer, input_sentence, max_len=50):\n",
    "    transformer_model.eval()  # Set model to evaluation mode\n",
    "    \n",
    "    # Tokenize the input English sentence\n",
    "    source_tokens = tokenizer.encode(input_sentence, out_type=int)\n",
    "    source_tokens = source_tokens[:max_len] + [PAD_IDX] * (max_len - len(source_tokens))  # Pad to max_len\n",
    "    source_input = torch.tensor(source_tokens).unsqueeze(0).to(training_device)  # Add batch dimension and move to device\n",
    "\n",
    "    # Generate translation using the model\n",
    "    with torch.no_grad():\n",
    "        output_tokens = transformer_model(source_input, source_input)  # Forward pass (translation)\n",
    "\n",
    "        # If output is logits, apply argmax to get token IDs\n",
    "        if isinstance(output_tokens, torch.Tensor):\n",
    "            output_tokens = torch.argmax(output_tokens, dim=-1)  # Get the predicted token IDs\n",
    "\n",
    "    # Decode token IDs to Urdu text\n",
    "    translated_text = tokenizer.decode(output_tokens.squeeze().tolist())  # Convert token IDs to text\n",
    "    return translated_text\n",
    "\n",
    "# Example usage\n",
    "english_input = \"In the name of Allah, the Most Gracious, the Most Merciful.\"\n",
    "urdu_translation = translate(transformer_model, tokenizer, english_input)\n",
    "print(\"Translation:\", urdu_translation)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source: And to them it was given that they should not kill them , but that they should be tormented five months : and their torment was as the torment of a scorpion , when he striketh a man .\n",
      "Reference: اور انہیں جان سے مارنے کا نہیں بلکہ پانچ مہینے تک لوگوں کو اذیت دینے کا اختیار دیا گیا اور ان کی اذیت ایسی تھی جیسے بچھو کے ڈنک مارنے سے آدمی کو ہوتی ہے ۔\n",
      "Hypothesis: کو کو کو کو کو کو کو کو کو کو کو کو کو کو کو کو کو کو کو کو کو کو کو کو کو کو کو کو کو کو کو کو کو کو کو کو کو کو کو کو کو کو کو کو کو کو کو کو کو کو\n",
      "BLEU: 0.0000, ROUGE-1: 0.0000, ROUGE-L: 0.0000\n",
      "\n",
      "Source: And in those days shall men seek death , and shall not find it ; and shall desire to die , and death shall flee from them .\n",
      "Reference: ان دنوں میں آدمی موت ڈھونڈیں گے مگر ہرگز نہ پائیں گے اور مرنے کی آرزو کریں گے اور موت ان سے بھاگے گی ۔\n",
      "Hypothesis: ۔ ۔ ۔ ۔ ۔ ۔ ۔ ۔ ۔ ۔ ۔ ۔ ۔ ۔ ۔ ۔ ۔ ۔ ۔ ۔ ۔ ۔ ۔ ۔ ۔ ۔ ۔ ۔ ۔ ۔ ۔ ۔ ۔ ۔ ۔ ۔ ۔ ۔ ۔ ۔ ۔ ۔ ۔ ۔ ۔ ۔ ۔ ۔ ۔ ۔\n",
      "BLEU: 0.0000, ROUGE-1: 0.0000, ROUGE-L: 0.0000\n",
      "\n",
      "Source: And the shapes of the locusts were like unto horses prepared unto battle ; and on their heads were as it were crowns like gold , and their faces were as the faces of men .\n",
      "Reference: اور ان ڈڈیوں کی صورتیں ان گھوڑوں کی سی تھیں جو لڑائی کے لئے تیار کئے گئے ہوں اور ان کے سروں پر گویا سونے کے تاج تھے اور ان کے چہرے آدمیوں کے سے تھے ۔\n",
      "Hypothesis: کے کے کے کے کے کے کے کے کے کے کے کے کے کے کے کے کے کے کے کے کے کے کے کے کے کے کے کے کے کے کے کے کے کے کے کے کے کے کے کے کے کے کے کے کے کے کے کے کے کے\n",
      "BLEU: 0.0000, ROUGE-1: 0.0000, ROUGE-L: 0.0000\n",
      "\n",
      "Source: And they had hair as the hair of women , and their teeth were as the teeth of lions .\n",
      "Reference: اور بال عورتوں کے سے تھے اور دانت ببر کے سے ۔\n",
      "Hypothesis: ۔ ۔ ۔ ۔ ۔ ۔ ۔ ۔ ۔ ۔ ۔ ۔ ۔ ۔ ۔ ۔ ۔ ۔ ۔ ۔ ۔ ۔ ۔ ۔ ۔ ۔ ۔ ۔ ۔ ۔ ۔ ۔ ۔ ۔ ۔ ۔ ۔ ۔ ۔ ۔ ۔ ۔ ۔ ۔ ۔ ۔ ۔ ۔ ۔ ۔\n",
      "BLEU: 0.0000, ROUGE-1: 0.0000, ROUGE-L: 0.0000\n",
      "\n",
      "Source: And they had breastplates , as it were breastplates of iron ; and the sound of their wings was as the sound of chariots of many horses running to battle .\n",
      "Reference: ان کے پاس لوہے کے سے بکتر تھے اور ان کے پروں کی آواز ایسی تھی جیسے رتھوں اور بہت سے گھوڑوں کی جو لڑائی میں دوڑتے ہوں ۔\n",
      "Hypothesis: لوگوں لوگوں لوگوں لوگوں لوگوں لوگوں لوگوں لوگوں لوگوں لوگوں لوگوں لوگوں لوگوں لوگوں لوگوں لوگوں لوگوں لوگوں لوگوں لوگوں کے کے کے کے کے کے کے کے کے کے کے کے کے کے کے کے کے لوگوں لوگوں لوگوں لوگوں لوگوں لوگوں لوگوں لوگوں لوگوں لوگوں لوگوں لوگوں لوگوں\n",
      "BLEU: 0.0000, ROUGE-1: 0.0000, ROUGE-L: 0.0000\n",
      "\n",
      "Average BLEU Score: 0.0000\n",
      "Average ROUGE-1 Score: 0.0000\n",
      "Average ROUGE-L Score: 0.0000\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import nltk\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from rouge_score import rouge_scorer\n",
    "import sentencepiece as spm\n",
    "\n",
    "\n",
    "# Initialize the tokenizer\n",
    "class Tokenizer:\n",
    "    def __init__(self, model_path, max_seq_len=50):\n",
    "        self.tokenizer = spm.SentencePieceProcessor()\n",
    "        self.tokenizer.load(model_path)\n",
    "        self.MAX_SEQ_LEN = max_seq_len\n",
    "        self.PAD_IDX = self.tokenizer.piece_to_id(\"<pad>\")\n",
    "        self.SOS_IDX = self.tokenizer.piece_to_id(\"<sos>\")\n",
    "        self.EOS_IDX = self.tokenizer.piece_to_id(\"<eos>\")\n",
    "\n",
    "    def tokenize_and_prepare(self, sentences):\n",
    "        tokenized = [self.tokenizer.encode(sentence, out_type=int) for sentence in sentences]\n",
    "        padded = [seq[:self.MAX_SEQ_LEN] + [self.PAD_IDX] * (self.MAX_SEQ_LEN - len(seq)) \n",
    "                  if len(seq) < self.MAX_SEQ_LEN else seq[:self.MAX_SEQ_LEN] for seq in tokenized]\n",
    "        return padded\n",
    "\n",
    "    def decode_tokens(self, tokens):\n",
    "        return self.tokenizer.decode(tokens)\n",
    "\n",
    "\n",
    "# Load test data\n",
    "def load_test_data(en_file, ur_file):\n",
    "    with open(en_file, 'r', encoding='utf-8') as f:\n",
    "        source_sentences = f.readlines()\n",
    "    with open(ur_file, 'r', encoding='utf-8') as f:\n",
    "        reference_sentences = f.readlines()\n",
    "    return source_sentences, reference_sentences\n",
    "\n",
    "\n",
    "# BLEU score calculation\n",
    "def calculate_bleu(reference, hypothesis):\n",
    "    reference = [nltk.word_tokenize(reference.strip())]\n",
    "    hypothesis = nltk.word_tokenize(hypothesis.strip())\n",
    "    return sentence_bleu(reference, hypothesis)\n",
    "\n",
    "\n",
    "# ROUGE score calculation\n",
    "def calculate_rouge(reference, hypothesis):\n",
    "    scorer = rouge_scorer.RougeScorer(['rouge1', 'rougeL'], use_stemmer=True)\n",
    "    return scorer.score(reference.strip(), hypothesis.strip())\n",
    "\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "def evaluate_model(model, tokenizer, en_file, ur_file, device):\n",
    "    model.eval()\n",
    "    source_sentences, reference_sentences = load_test_data(en_file, ur_file)\n",
    "\n",
    "    total_bleu = 0\n",
    "    total_rouge1 = 0\n",
    "    total_rougeL = 0\n",
    "    scorer = rouge_scorer.RougeScorer(['rouge1', 'rougeL'], use_stemmer=True)\n",
    "\n",
    "    for i, (source, reference) in enumerate(zip(source_sentences, reference_sentences)):\n",
    "        # Tokenize and prepare input\n",
    "        source_tensor = torch.tensor(tokenizer.tokenize_and_prepare([source]), dtype=torch.long).to(device)\n",
    "        tgt_tensor = torch.full((source_tensor.size(0), tokenizer.MAX_SEQ_LEN), tokenizer.SOS_IDX, dtype=torch.long).to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            # Get model prediction\n",
    "            output = model(source_tensor, tgt_tensor)\n",
    "            prediction_ids = output.argmax(dim=-1).squeeze().tolist()\n",
    "            hypothesis = tokenizer.decode_tokens(prediction_ids)\n",
    "\n",
    "        # Calculate BLEU score\n",
    "        bleu_score = calculate_bleu(reference, hypothesis)\n",
    "        total_bleu += bleu_score\n",
    "\n",
    "        # Calculate ROUGE scores\n",
    "        rouge_scores = calculate_rouge(reference, hypothesis)\n",
    "        total_rouge1 += rouge_scores['rouge1'].fmeasure\n",
    "        total_rougeL += rouge_scores['rougeL'].fmeasure\n",
    "\n",
    "        # Print example predictions for the first 5\n",
    "        if i < 5:\n",
    "            print(f\"Source: {source.strip()}\")\n",
    "            print(f\"Reference: {reference.strip()}\")\n",
    "            print(f\"Hypothesis: {hypothesis.strip()}\")\n",
    "            print(f\"BLEU: {bleu_score:.4f}, ROUGE-1: {rouge_scores['rouge1'].fmeasure:.4f}, ROUGE-L: {rouge_scores['rougeL'].fmeasure:.4f}\\n\")\n",
    "\n",
    "    # Calculate and print average scores\n",
    "    num_samples = len(source_sentences)\n",
    "    avg_bleu = total_bleu / num_samples\n",
    "    avg_rouge1 = total_rouge1 / num_samples\n",
    "    avg_rougeL = total_rougeL / num_samples\n",
    "\n",
    "    print(f\"Average BLEU Score: {avg_bleu:.4f}\")\n",
    "    print(f\"Average ROUGE-1 Score: {avg_rouge1:.4f}\")\n",
    "    print(f\"Average ROUGE-L Score: {avg_rougeL:.4f}\")\n",
    "\n",
    "    return avg_bleu, avg_rouge1, avg_rougeL\n",
    "\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    en_file = \"umc005-corpus/bible/test.en\"\n",
    "    ur_file = \"umc005-corpus/bible/test.ur\"\n",
    "\n",
    "    tokenizer = Tokenizer(model_path=\"urdu_english_bpe_parallel.model\")\n",
    "    \n",
    "    # Assuming the trained model is loaded as `model`\n",
    "    avg_bleu, avg_rouge1, avg_rougeL = evaluate_model(model, tokenizer, en_file, ur_file, device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Loss values from your training output (replace with the actual loss values you have)\n",
    "train_losses = [\n",
    "    5.847795763905369, 4.914523594290463, 4.5751000347600055, 4.352262926599873,\n",
    "    4.184005458959892, 4.042845732934439, 3.918032662388104, 3.8128471590867683,\n",
    "    3.7139017724457073, 3.621993897566155, 3.534025015350598, 3.4561936408325806,\n",
    "    3.383977654051425, 3.312921400176945, 3.2490196408353635, 3.184731990588245,\n",
    "    3.1271846369531615, 3.0695784538717414, 3.020777788633731, 2.9712336649378734\n",
    "]\n",
    "\n",
    "# Plotting the training loss curve\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(range(1, len(train_losses) + 1), train_losses, label='Training Loss', color='blue')\n",
    "\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Loss Curve')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine Tuning Pretrained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9eb87dec81442818d41a22d0afe613a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\javer\\Anaconda3\\envs\\pymc5_env\\Lib\\site-packages\\huggingface_hub\\file_download.py:157: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\javer\\.cache\\huggingface\\hub\\models--bert-base-uncased. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.7043259739875793\n",
      "Epoch 2, Loss: 0.75309818983078\n",
      "Epoch 3, Loss: 0.6424257159233093\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import BertForSequenceClassification, BertTokenizer\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "# Define the BERT model and tokenizer setup in a separate class\n",
    "class BertModel:\n",
    "    def __init__(self, model_name='bert-base-uncased', num_labels=2):\n",
    "        self.model = BertForSequenceClassification.from_pretrained(model_name, num_labels=num_labels)\n",
    "        self.tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "\n",
    "    def get_model(self):\n",
    "        return self.model\n",
    "\n",
    "    def get_tokenizer(self):\n",
    "        return self.tokenizer\n",
    "\n",
    "\n",
    "# Dataset class for preparing the data\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_length=32):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        text = self.texts[item]\n",
    "        label = self.labels[item]\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            text, \n",
    "            add_special_tokens=True, \n",
    "            max_length=self.max_length, \n",
    "            padding='max_length', \n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'labels': torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "\n",
    "# Define the training loop function\n",
    "def train_model(model, train_loader, optimizer, loss_fn, epochs=3, device='cpu'):\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        for batch in train_loader:\n",
    "            # Move data to device\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            loss = outputs.loss\n",
    "            logits = outputs.logits\n",
    "            \n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        print(f\"Epoch {epoch + 1}, Loss: {total_loss / len(train_loader):.4f}\")\n",
    "\n",
    "\n",
    "# Example usage of the refactored code\n",
    "if __name__ == \"__main__\":\n",
    "    # Sample data\n",
    "    texts = [\"I love machine learning\", \"I hate bugs\"]\n",
    "    labels = [1, 0]  # 1 = positive, 0 = negative\n",
    "\n",
    "    # Initialize the BERT model and tokenizer\n",
    "    bert_model = BertModel()\n",
    "    model = bert_model.get_model().to(device)  # Ensure to use the appropriate device (CPU or GPU)\n",
    "    tokenizer = bert_model.get_tokenizer()\n",
    "\n",
    "    # Prepare dataset and dataloader\n",
    "    dataset = MyDataset(texts, labels, tokenizer, max_length=32)\n",
    "    train_loader = DataLoader(dataset, batch_size=2)\n",
    "\n",
    "    # Set up optimizer and loss function\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=2e-5)\n",
    "    loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    # Train the model\n",
    "    train_model(model, train_loader, optimizer, loss_fn, epochs=3, device=device)\n",
    "\n",
    "    # Evaluation (for example, on a validation set)\n",
    "    model.eval()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pymc5_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
